"""
SQLæ–‡ä»¶å¤„ç†å™¨ - ä»SQLæ–‡ä»¶ä¸­æå–å…ƒæ•°æ®å¹¶å­˜å‚¨åˆ°SQLiteæ•°æ®åº“

åŠŸèƒ½ï¼š
1. è¯»å–SQLæ–‡ä»¶å†…å®¹
2. è§£æSQLè¯­å¥ï¼ˆDDLå’ŒDMLï¼‰
3. æå–è¡¨å’Œå­—æ®µå…ƒæ•°æ®
4. å¤„ç†ä¸ç°æœ‰æ•°æ®çš„å†²çª
5. å­˜å‚¨åˆ°dw_metadata.dbæ•°æ®åº“

ä½œè€…ï¼šAI Assistant
åˆ›å»ºæ—¶é—´ï¼š2025-01-23
"""

import sqlite3
import os
import json
import subprocess
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Set
import sqlglot
from sqlglot import exp
import networkx as nx
from metadata_extractor import extract_ddl_metadata, extract_sql_metadata, _classify_statement_type


def process_sql_file(
    sql_file_path: str,
    dialect: str = None,
    db_path: str = 'dw_metadata.db'
) -> Tuple[bool, str]:
    """
    å¤„ç†SQLæ–‡ä»¶å¹¶å­˜å‚¨å…ƒæ•°æ®åˆ°æ•°æ®åº“
    
    Args:
        sql_file_path: SQLæ–‡ä»¶è·¯å¾„
        dialect: SQLæ–¹è¨€ï¼ˆå¦‚'mysql', 'teradata', 'postgres'ç­‰ï¼‰
        db_path: SQLiteæ•°æ®åº“è·¯å¾„ï¼Œé»˜è®¤ä¸º'dw_metadata.db'
    
    Returns:
        (True, '') - æˆåŠŸ
        (False, 'é”™è¯¯åŸå› ') - å¤±è´¥
    """
    try:
        # 1. è¯»å–SQLæ–‡ä»¶
        print(f"ğŸ“– æ­£åœ¨è¯»å–SQLæ–‡ä»¶: {sql_file_path}")
        if not os.path.exists(sql_file_path):
            return False, f"æ–‡ä»¶ä¸å­˜åœ¨: {sql_file_path}"
        
        with open(sql_file_path, 'r', encoding='utf-8') as f:
            sql_content = f.read()
        
        # è‹¥æ–‡ä»¶ä¸ºç©ºï¼Œåˆ™ç›´æ¥è¿”å›æˆåŠŸ
        if not sql_content.strip():
            return True, ""
        
        # 2. è§£æSQLè¯­å¥
        print(f"ğŸ” æ­£åœ¨è§£æSQLè¯­å¥...")
        try:
            parsed_statements = sqlglot.parse(sql_content, dialect=dialect)
        except Exception as e:
            return False, f"SQLè§£æå¤±è´¥: {str(e)}"
        
        if not parsed_statements:
            return False, "æœªèƒ½è§£æå‡ºä»»ä½•SQLè¯­å¥"
        
        print(f"âœ… æˆåŠŸè§£æ {len(parsed_statements)} æ¡SQLè¯­å¥")
        
        # 3. æå–å…ƒæ•°æ®
        print(f"ğŸ“Š æ­£åœ¨æå–å…ƒæ•°æ®...")
        extracted_data = []
        
        for idx, parsed_sql in enumerate(parsed_statements, 1):
            if parsed_sql is None:
                continue
                
            try:
                # ä½¿ç”¨_classify_statement_typeè·å–ç»†ç²’åº¦ç±»å‹
                statement_type = _classify_statement_type(parsed_sql)
                
                # æ ¹æ®ç±»å‹åˆ¤æ–­æ˜¯DDLè¿˜æ˜¯DML
                ddl_types = {'CREATE_TABLE', 'CREATE_TABLE_AS', 'CREATE_VIEW'}
                dml_types = {'INSERT_SELECT', 'INSERT_VALUES', 'UPDATE', 'MERGE'}
                
                if statement_type in ddl_types:
                    print(f"  [{idx}] DDLè¯­å¥ ({statement_type}) - {type(parsed_sql).__name__}")
                    metadata = extract_ddl_metadata(parsed_sql.sql(dialect=dialect), dialect=dialect)
                    metadata['statement_type'] = statement_type
                    metadata['_type'] = 'DDL'
                    metadata['_ast'] = parsed_sql
                    extracted_data.append(metadata)
                    
                elif statement_type in dml_types:
                    print(f"  [{idx}] DMLè¯­å¥ ({statement_type}) - {type(parsed_sql).__name__}")
                    metadata = extract_sql_metadata(parsed_sql.sql(dialect=dialect), dialect=dialect)
                    metadata['statement_type'] = statement_type
                    metadata['_type'] = 'DML'
                    metadata['_ast'] = parsed_sql
                    extracted_data.append(metadata)
                    
                else:
                    print(f"  [{idx}] è·³è¿‡è¯­å¥ ({statement_type}) - {type(parsed_sql).__name__} (ä¸æ”¯æŒçš„ç±»å‹)")
                    
            except Exception as e:
                return False, f"æå–ç¬¬{idx}æ¡SQLå…ƒæ•°æ®å¤±è´¥: {str(e)}"
        
        if not extracted_data:
            return False, "æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„å…ƒæ•°æ®"
        
        print(f"âœ… æˆåŠŸæå– {len(extracted_data)} æ¡å…ƒæ•°æ®")
        
        # 4. æ•´åˆæ•°æ®ï¼ˆæŒ‰è¡¨åˆ†ç»„ï¼‰
        print(f"ğŸ”„ æ­£åœ¨æ•´åˆæ•°æ®...")
        tables_data = _consolidate_metadata(extracted_data)
        print(f"âœ… æ•´åˆåå…± {len(tables_data)} ä¸ªè¡¨")
        
        # 5. æ„å»ºä¾èµ–å›¾å¹¶è¯†åˆ«ç›®æ ‡è¡¨ï¼ˆéœ€è¦åœ¨å¤„ç†è¡¨æ•°æ®å‰ç”Ÿæˆscript_idï¼‰
        print(f"\nğŸ“Š æ­£åœ¨æ„å»ºä¾èµ–å›¾...")
        try:
            dependency_graph = _build_dependency_graph(extracted_data)
            print(f"âœ… ä¾èµ–å›¾æ„å»ºå®Œæˆ: {len(dependency_graph.nodes())} ä¸ªèŠ‚ç‚¹, {len(dependency_graph.edges())} æ¡è¾¹")
            
            # 6. ä¿å­˜ä¾èµ–å›¾åˆ°æ–‡ä»¶
            graph_file_path = _save_dependency_graph(sql_file_path, dependency_graph)
            print(f"âœ… ä¾èµ–å›¾å·²ä¿å­˜åˆ°: {graph_file_path}")
            
            # 7. è¯†åˆ«ç›®æ ‡è¡¨å’Œæ¥æºè¡¨
            target_tables, source_tables = _identify_target_and_source_tables(dependency_graph, extracted_data)
            print(f"âœ… è¯†åˆ«åˆ°ç›®æ ‡è¡¨: {target_tables}")
            print(f"âœ… è¯†åˆ«åˆ°æ¥æºè¡¨: {source_tables}")
            
            # æ£€æŸ¥ç›®æ ‡è¡¨æ•°é‡
            if len(target_tables) == 0:
                return False, "æœªèƒ½è¯†åˆ«åˆ°ç›®æ ‡è¡¨"
            
            # 8. ç”Ÿæˆscript_idï¼ˆåªä½¿ç”¨è„šæœ¬åï¼Œä¸å«æ‰©å±•åï¼‰
            script_name = os.path.splitext(os.path.basename(sql_file_path))[0]
            script_id = script_name
            
            print(f"âœ… ç”Ÿæˆè„šæœ¬ID: {script_id}")
            print(f"   è„šæœ¬æ“ä½œ {len(target_tables)} ä¸ªç›®æ ‡è¡¨")
            
        except Exception as e:
            return False, f"æ„å»ºä¾èµ–å›¾å¤±è´¥: {str(e)}"
        
        # 9. è¿æ¥æ•°æ®åº“å¹¶å¤„ç†å†²çª
        print(f"\nğŸ’¾ æ­£åœ¨è¿æ¥æ•°æ®åº“: {db_path}")
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row  # ä½¿ç»“æœå¯ä»¥é€šè¿‡åˆ—åè®¿é—®
        
        try:
            # ä½¿ç”¨äº‹åŠ¡
            cursor = conn.cursor()
            cursor.execute("BEGIN TRANSACTION")
            
            # 10. å¤„ç†æ¯ä¸ªè¡¨çš„æ•°æ®ï¼ˆä¼ å…¥script_idç”¨äºä¸´æ—¶è¡¨ï¼‰
            for table_key, table_data in tables_data.items():
                print(f"\nğŸ“‹ å¤„ç†è¡¨: {table_key}")
                try:
                    _process_table_data(cursor, table_data, script_id)
                except Exception as e:
                    conn.rollback()
                    return False, f"å¤„ç†è¡¨ {table_key} æ—¶å‡ºé”™: {str(e)}"
            
            # 11. å¡«å……sql_scriptsã€script_statementsã€data_lineage_detailè¡¨
            print(f"\nğŸ“ æ­£åœ¨å¡«å……è„šæœ¬ä¿¡æ¯...")
            _populate_script_tables(
                cursor, 
                sql_file_path, 
                sql_content,
                target_tables,  # ä¼ å…¥ç›®æ ‡è¡¨é›†åˆï¼ˆå¯èƒ½å¤šä¸ªï¼‰
                source_tables,
                extracted_data,
                dependency_graph,
                parsed_statements,  # æ–°å¢ï¼šä¼ é€’è§£æåçš„è¯­å¥
                script_id
            )
            print(f"âœ… è„šæœ¬ä¿¡æ¯å·²ä¿å­˜")
            
            # 12. æ›´æ–°å…¨å±€è¡€ç¼˜å›¾
            print(f"\nğŸŒ æ­£åœ¨æ›´æ–°å…¨å±€è¡€ç¼˜å›¾...")
            _update_global_lineage(
                sql_file_path,
                target_tables,  # ä¼ å…¥ç›®æ ‡è¡¨é›†åˆï¼ˆå¯èƒ½å¤šä¸ªï¼‰
                source_tables
            )
            print(f"âœ… å…¨å±€è¡€ç¼˜å›¾å·²æ›´æ–°")
            
            # æäº¤äº‹åŠ¡
            conn.commit()
            print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“")
            
        finally:
            conn.close()
        
        return True, ""
        
    except Exception as e:
        return False, f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}"


def _identify_statement_type(parsed_sql: exp.Expression) -> str:
    """
    è·å–æ›´ç²¾ç»†çš„è¯­å¥ç±»å‹

    Args:
        parsed_sql: sqlglotè§£æåçš„AST

    Returns:
        'CREATE_TABLE' - CREATE TABLEè¯­å¥ï¼ˆå®Œæ•´å­—æ®µå®šä¹‰ï¼‰
        'INSERT_EXPLICIT' - INSERT(col1,col2)è¯­å¥ï¼ˆæ˜¾å¼æŒ‡å®šåˆ—åï¼‰
        'INSERT_VALUES' - INSERT VALUESè¯­å¥ï¼ˆæœªæ˜¾å¼æŒ‡å®šåˆ—åï¼‰
        'UPDATE' - UPDATEè¯­å¥ï¼ˆéƒ¨åˆ†å­—æ®µæ›´æ–°ï¼‰
        'MERGE' - MERGEè¯­å¥ï¼ˆå¤æ‚æ“ä½œï¼‰
        'OTHER' - å…¶ä»–ç±»å‹
    """
    if isinstance(parsed_sql, exp.Create):
        return 'CREATE_TABLE'
    elif isinstance(parsed_sql, exp.Insert):
        # æ£€æŸ¥æ˜¯å¦æ˜¾å¼æŒ‡å®šäº†åˆ—å
        schema = parsed_sql.find(exp.Schema)
        if schema and schema.expressions:
            return 'INSERT_EXPLICIT'
        else:
            return 'INSERT_VALUES'
    elif isinstance(parsed_sql, exp.Update):
        return 'UPDATE'
    elif isinstance(parsed_sql, exp.Merge):
        return 'MERGE'
    else:
        return 'OTHER'


def _consolidate_metadata(extracted_data: List[Dict]) -> Dict[str, Dict]:
    """
    æ•´åˆå…ƒæ•°æ®ï¼ˆæŒ‰è¡¨åˆ†ç»„ï¼‰
    
    å¤„ç†ç›®æ ‡è¡¨å’Œæ¥æºè¡¨ï¼Œç¡®ä¿æ‰€æœ‰å¼•ç”¨çš„è¡¨éƒ½è¢«è®°å½•
    
    Args:
        extracted_data: æå–çš„å…ƒæ•°æ®åˆ—è¡¨
    
    Returns:
        æŒ‰è¡¨åˆ†ç»„çš„æ•°æ®å­—å…¸ï¼Œkeyä¸º(schema_name, table_name)
    """
    tables_data = {}
    
    for metadata in extracted_data:
        target_table = metadata['target_table']

        # è·å–è¯­å¥ç±»å‹
        stmt_type = metadata.get('statement_type', _classify_statement_type(metadata['_ast']))

        # 1. å¤„ç†ç›®æ ‡è¡¨
        schema_name = target_table.get('schema_nm', '') or ''
        table_name = target_table.get('tbl_en_nm', '')

        if table_name:
            # ä½¿ç”¨(schema_name, table_name)ä½œä¸ºkey
            table_key = (schema_name, table_name)

            if table_key not in tables_data:
                # ç¡®å®šè¡¨ç±»å‹
                table_type = _determine_table_type(metadata['_ast'], schema_name)
                tables_data[table_key] = {
                    'schema_name': schema_name,
                    'table_name': table_name,
                    'table_cn_name': target_table.get('tbl_cn_nm', ''),
                    'table_type': table_type,
                    'data_source': stmt_type,
                    'columns': [],
                    'ast': metadata['_ast']
                }

            # æ›´æ–°data_sourceä¸ºæ›´å…·ä½“çš„ç±»å‹
            tables_data[table_key]['data_source'] = stmt_type
            
            # æ•´åˆå­—æ®µä¿¡æ¯
            if 'target_columns' in metadata and metadata['target_columns']:
                for col in metadata['target_columns']:
                    # æ£€æŸ¥å­—æ®µæ˜¯å¦å·²å­˜åœ¨
                    existing_col = None
                    for existing in tables_data[table_key]['columns']:
                        if existing['col_en_nm'] == col.get('col_en_nm'):
                            existing_col = existing
                            break
                    
                    if existing_col:
                        # åˆå¹¶å­—æ®µä¿¡æ¯ï¼ˆä¼˜å…ˆä¿ç•™å»ºè¡¨è¯­å¥ä¿¡æ¯ï¼‰
                        ddl_types = {'CREATE_TABLE', 'CREATE_TABLE_AS', 'CREATE_VIEW'}
                        if stmt_type in ddl_types:
                            # å»ºè¡¨è¯­å¥ä¼˜å…ˆï¼Œè¦†ç›–åŸæœ‰ä¿¡æ¯
                            for key, value in col.items():
                                if value or key in ['is_null', 'is_pri_key', 'is_foreign_key']:
                                    existing_col[key] = value
                        else:
                            # DMLè¡¥å……ä¿¡æ¯
                            for key, value in col.items():
                                if value and not existing_col.get(key):
                                    existing_col[key] = value
                    else:
                        # æ–°å­—æ®µ
                        tables_data[table_key]['columns'].append(col)
        
        # 2. å¤„ç†æ¥æºè¡¨ï¼ˆä»…å½“æ˜¯DMLæˆ–æœ‰source_tablesæ—¶ï¼‰
        if 'source_tables' in metadata and metadata['source_tables']:
            for source_table in metadata['source_tables']:
                src_schema = source_table.get('schema_nm', '') or ''
                src_table = source_table.get('tbl_en_nm', '')
                
                if not src_table:
                    continue
                
                src_key = (src_schema, src_table)
                
                # å¦‚æœæ¥æºè¡¨è¿˜æœªè®°å½•ï¼Œæ·»åŠ ä¸ºå¤–éƒ¨è¡¨
                if src_key not in tables_data:
                    tables_data[src_key] = {
                        'schema_name': src_schema,
                        'table_name': src_table,
                        'table_cn_name': '',
                        'table_type': 'TABLE',  # é»˜è®¤ä¸ºTABLEç±»å‹
                        'data_source': 'EXTERNAL',  # æ ‡è®°ä¸ºå¤–éƒ¨è¡¨
                        'columns': [],
                        'ast': None
                    }
    
    return tables_data


def get_conflict_strategy(existing_type: str, new_type: str) -> str:
    """
    è·å–å†²çªå¤„ç†ç­–ç•¥

    Args:
        existing_type: æ•°æ®åº“ä¸­ç°æœ‰çš„è¯­å¥ç±»å‹
        new_type: æ–°çš„è¯­å¥ç±»å‹

    Returns:
        å†²çªå¤„ç†ç­–ç•¥: 'ERROR', 'KEEP_CREATE_TABLE', 'SUPPLEMENT_CHINESE_NAMES', 'MERGE_INFO'
    """
    # å»ºè¡¨è¯­å¥ä¼˜å…ˆçº§æœ€é«˜ï¼ˆåŒ…æ‹¬CREATE_TABLE, CREATE_TABLE_AS, CREATE_VIEWï¼‰
    ddl_types = {'CREATE_TABLE', 'CREATE_TABLE_AS', 'CREATE_VIEW'}
    
    if existing_type in ddl_types or new_type in ddl_types:
        # å¦‚æœä¸¤ä¸ªéƒ½æ˜¯å»ºè¡¨è¯­å¥ï¼Œä¸å…è®¸é‡å¤
        if existing_type in ddl_types and new_type in ddl_types:
            if existing_type == new_type:
                return 'ERROR'  # ä¸å…è®¸é‡å¤å»ºè¡¨
            else:
                # ä¸åŒç±»å‹çš„å»ºè¡¨è¯­å¥ï¼ŒCREATE_TABLEä¼˜å…ˆ
                if existing_type == 'CREATE_TABLE' or new_type == 'CREATE_TABLE':
                    return 'KEEP_CREATE_TABLE'
                # å…¶ä»–æƒ…å†µï¼Œä¿ç•™ç°æœ‰çš„
                return 'KEEP_CREATE_TABLE'

        # å»ºè¡¨è¯­å¥æ€»æ˜¯ä¼˜å…ˆ
        return 'KEEP_CREATE_TABLE'

    # å…¶ä»–è¯­å¥çš„åˆå¹¶ç­–ç•¥
    merge_strategies = {
        # DDL vs å…¶ä»–
        ('CREATE_TABLE_AS', 'INSERT_SELECT'): 'SUPPLEMENT_CHINESE_NAMES',
        ('CREATE_TABLE_AS', 'INSERT_VALUES'): 'SUPPLEMENT_CHINESE_NAMES',
        ('CREATE_TABLE_AS', 'UPDATE'): 'SUPPLEMENT_CHINESE_NAMES',
        ('CREATE_TABLE_AS', 'MERGE'): 'SUPPLEMENT_CHINESE_NAMES',

        # CREATE VIEW vs å…¶ä»–
        ('CREATE_VIEW', 'INSERT_SELECT'): 'SUPPLEMENT_CHINESE_NAMES',
        ('CREATE_VIEW', 'INSERT_VALUES'): 'SUPPLEMENT_CHINESE_NAMES',
        ('CREATE_VIEW', 'UPDATE'): 'SUPPLEMENT_CHINESE_NAMES',
        ('CREATE_VIEW', 'MERGE'): 'SUPPLEMENT_CHINESE_NAMES',

        # éå»ºè¡¨è¯­å¥ä¹‹é—´çš„åˆå¹¶
        ('INSERT_SELECT', 'INSERT_SELECT'): 'MERGE_INFO',
        ('INSERT_SELECT', 'INSERT_VALUES'): 'MERGE_INFO',
        ('INSERT_SELECT', 'UPDATE'): 'MERGE_INFO',
        ('INSERT_SELECT', 'MERGE'): 'MERGE_INFO',

        ('INSERT_VALUES', 'INSERT_VALUES'): 'MERGE_INFO',
        ('INSERT_VALUES', 'UPDATE'): 'MERGE_INFO',
        ('INSERT_VALUES', 'MERGE'): 'MERGE_INFO',

        ('UPDATE', 'UPDATE'): 'MERGE_INFO',
        ('UPDATE', 'MERGE'): 'MERGE_INFO',

        ('MERGE', 'MERGE'): 'MERGE_INFO',
    }

    return merge_strategies.get((existing_type, new_type), 'MERGE_INFO')


def _process_table_data(cursor: sqlite3.Cursor, table_data: Dict, script_id: str = None):
    """
    å¤„ç†å•ä¸ªè¡¨çš„æ•°æ®ï¼ˆåŒ…æ‹¬å†²çªæ£€æµ‹å’Œåˆå¹¶ï¼‰
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        table_data: è¡¨æ•°æ®å­—å…¸
    
    Raises:
        Exception: å½“æ£€æµ‹åˆ°ä¸å…è®¸çš„å†²çªæ—¶
    """
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    table_cn_name = table_data['table_cn_name']
    data_source = table_data['data_source']
    table_type = table_data['table_type']
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    is_tmp_table = (table_type == 'TMP_TABLE')
    # script_id: å®ä½“è¡¨ä¸ºNoneï¼Œä¸´æ—¶è¡¨ä¸ºå®é™…å€¼ï¼ˆç”¨äºIDç”Ÿæˆå’Œé€»è¾‘åˆ¤æ–­ï¼‰
    current_script_id = script_id if is_tmp_table else None

    # ç”Ÿæˆè¡¨IDï¼ˆä¸´æ—¶è¡¨éœ€è¦ä¼ å…¥script_idï¼‰
    table_id = _generate_table_id(schema_name, table_name, current_script_id)

    # æŸ¥è¯¢æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥è¡¨
    cursor.execute("""
        SELECT id, schema_name, table_name, table_type, description,
               data_source, refresh_frequency, row_count, data_size_mb,
               last_updated, created_at, script_id
        FROM tables
        WHERE id = ?
    """, (table_id,))

    existing_table = cursor.fetchone()
    
    if existing_table:
        print(f"  âš ï¸  è¡¨å·²å­˜åœ¨ï¼Œæ£€æµ‹å†²çª...")
        existing_data_source = existing_table['data_source']

        # å¦‚æœæ–°æ•°æ®æ˜¯EXTERNALï¼Œè·³è¿‡ï¼ˆå·²æœ‰ä»»ä½•å®šä¹‰éƒ½ä¼˜å…ˆï¼‰
        if data_source == 'EXTERNAL':
            print(f"  â­ï¸  è¡¨å·²å­˜åœ¨ï¼Œè·³è¿‡å¤–éƒ¨è¡¨åˆ›å»º")
            return

        # å¦‚æœå·²å­˜åœ¨çš„æ˜¯EXTERNALï¼Œç”¨æ–°æ•°æ®è¦†ç›–
        if existing_data_source == 'EXTERNAL':
            print(f"  ğŸ”„ ç”¨å®é™…å®šä¹‰è¦†ç›–å¤–éƒ¨è¡¨è®°å½•")
            _update_table_with_statement(cursor, table_id, table_data, current_script_id, data_source)
            return

        # è·å–å†²çªå¤„ç†ç­–ç•¥
        strategy = get_conflict_strategy(existing_data_source, data_source)
        print(f"  ğŸ”„ å†²çªç­–ç•¥: {strategy}")

        if strategy == 'ERROR':
            raise Exception(f"è¡¨ {schema_name}.{table_name} å†²çªä¸å…è®¸: {existing_data_source} vs {data_source}")

        elif strategy == 'KEEP_CREATE_TABLE':
            # å»ºè¡¨è¯­å¥ä¼˜å…ˆï¼Œè¦†ç›–å…¶ä»–å®šä¹‰
            print(f"  ğŸ”„ å»ºè¡¨è¯­å¥ä¼˜å…ˆè¦†ç›–")
            _update_table_with_statement(cursor, table_id, table_data, current_script_id, data_source)

        elif strategy == 'SUPPLEMENT_CHINESE_NAMES':
            # åªè¡¥å……ä¸­æ–‡åï¼Œä¸æ£€æŸ¥å­—æ®µå­˜åœ¨æ€§
            print(f"  â• åªè¡¥å……ä¸­æ–‡åä¿¡æ¯")
            _supplement_chinese_names_only(cursor, table_id, table_data, current_script_id)

        elif strategy == 'MERGE_INFO':
            # æ­£å¸¸åˆå¹¶ä¿¡æ¯
            print(f"  ğŸ”€ åˆå¹¶å­—æ®µä¿¡æ¯")
            _merge_statement_info(cursor, table_id, table_data, current_script_id, existing_data_source, data_source)

    else:
        print(f"  âœ¨ æ–°å»ºè¡¨è®°å½•")
        _insert_new_table(cursor, table_data, current_script_id)


def _generate_table_id(schema_name: str, table_name: str, script_id: str = None) -> str:
    """
    ç”Ÿæˆè¡¨ID
    è§„åˆ™ï¼š
    - æœ‰schemaä¸”æœ‰script_idï¼ˆä¸´æ—¶è¡¨ï¼‰: {SCHEMA_NAME}__{TABLE_NAME}__{SCRIPT_ID}
    - æœ‰schemaæ— script_idï¼ˆå®ä½“è¡¨ï¼‰: {SCHEMA_NAME}__{TABLE_NAME}__
    - æ— schemaæœ‰script_idï¼ˆä¸´æ—¶è¡¨ï¼‰: __{TABLE_NAME}__{SCRIPT_ID}
    - æ— schemaæ— script_idï¼ˆä¸´æ—¶è¡¨ï¼Œæ— è„šæœ¬ï¼‰: __{TABLE_NAME}__
    """
    if schema_name:
        if script_id:
            return f"{schema_name}__{table_name}__{script_id}"
        else:
            return f"{schema_name}__{table_name}__"
    else:
        if script_id:
            return f"__{table_name}__{script_id}"
        else:
            return f"__{table_name}__"


def _generate_column_id(schema_name: str, table_name: str, column_name: str, script_id: str = None) -> str:
    """
    ç”Ÿæˆå­—æ®µID
    è§„åˆ™ï¼š
    - æœ‰schemaä¸”æœ‰script_idï¼ˆä¸´æ—¶è¡¨ï¼‰: {SCHEMA_NAME}__{TABLE_NAME}__{SCRIPT_ID}__{COLUMN_NAME}
    - æœ‰schemaæ— script_idï¼ˆå®ä½“è¡¨ï¼‰: {SCHEMA_NAME}__{TABLE_NAME}____{COLUMN_NAME}
    - æ— schemaæœ‰script_idï¼ˆä¸´æ—¶è¡¨ï¼‰: __{TABLE_NAME}__{SCRIPT_ID}__{COLUMN_NAME}
    - æ— schemaæ— script_idï¼ˆä¸´æ—¶è¡¨ï¼Œæ— è„šæœ¬ï¼‰: __{TABLE_NAME}____{COLUMN_NAME}
    """
    if schema_name:
        if script_id:
            return f"{schema_name}__{table_name}__{script_id}__{column_name}"
        else:
            return f"{schema_name}__{table_name}____{column_name}"
    else:
        if script_id:
            return f"__{table_name}__{script_id}__{column_name}"
        else:
            return f"__{table_name}____{column_name}"


def _determine_table_type(ast: exp.Expression, schema_name: str) -> str:
    """
    ç¡®å®šè¡¨ç±»å‹
    
    Args:
        ast: SQLè¯­å¥çš„AST
        schema_name: schemaåç§°
    
    Returns:
        'TABLE', 'VIEW', 'TMP_TABLE'
    """
    if isinstance(ast, exp.Create):
        # æ£€æŸ¥æ˜¯å¦æ˜¯VIEW
        if hasattr(ast, 'kind') and ast.kind and 'VIEW' in str(ast.kind).upper():
            return 'VIEW'
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶è¡¨
        if hasattr(ast, 'args'):
            # TEMPORARY TABLE
            if ast.args.get('temporary'):
                return 'TMP_TABLE'
            
            # VOLATILE TABLE (Teradata) - æ£€æŸ¥argsä¸­çš„volatileæ ‡å¿—
            if ast.args.get('volatile'):
                return 'TMP_TABLE'
            
            # æ£€æŸ¥propertiesä¸­æ˜¯å¦åŒ…å«VOLATILEæˆ–TEMPORARY
            if 'properties' in ast.args and ast.args['properties']:
                properties = ast.args['properties']
                if hasattr(properties, 'expressions'):
                    for prop in properties.expressions:
                        prop_str = str(type(prop).__name__).upper()
                        prop_value = str(prop).upper()
                        # æ£€æŸ¥StabilityProperty: VOLATILE æˆ–åŒ…å«TEMPORARYçš„å±æ€§
                        if 'VOLATILE' in prop_str or 'VOLATILE' in prop_value:
                            return 'TMP_TABLE'
                        if 'TEMPORARY' in prop_str or 'TEMPORARY' in prop_value:
                            return 'TMP_TABLE'
            
            # æ£€æŸ¥kindä¸­æ˜¯å¦åŒ…å«VOLATILEæˆ–TEMPORARYå…³é”®è¯
            if hasattr(ast, 'kind') and ast.kind:
                kind_str = str(ast.kind).upper()
                if 'VOLATILE' in kind_str or 'TEMPORARY' in kind_str:
                    return 'TMP_TABLE'
        
        return 'TABLE'
    
    elif isinstance(ast, (exp.Insert, exp.Update, exp.Merge)):
        # DMLè¯­å¥ï¼šæœ‰schema_nameåˆ™ä¸ºTABLEï¼Œå¦åˆ™ä¸ºTMP_TABLE
        if schema_name:
            return 'TABLE'
        else:
            return 'TMP_TABLE'
    
    return 'TABLE'


def _update_table_with_statement(cursor: sqlite3.Cursor, table_id: str, table_data: Dict, script_id: str = None, new_data_source: str = None):
    """
    ç”¨æ–°è¯­å¥å®Œå…¨è¦†ç›–è¡¨ä¿¡æ¯

    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        table_id: è¡¨ID
        table_data: æ–°çš„è¡¨æ•°æ®
        script_id: è„šæœ¬ID
        new_data_source: æ–°çš„æ•°æ®æºç±»å‹
    """
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    table_cn_name = table_data['table_cn_name']
    table_type = table_data['table_type']

    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    is_tmp_table = (table_type == 'TMP_TABLE')
    current_script_id = script_id if is_tmp_table else ''

    # æ›´æ–°è¡¨åŸºæœ¬ä¿¡æ¯
    cursor.execute("""
        UPDATE tables
        SET database_id = ?, schema_name = ?, table_name = ?, table_type = ?,
            description = ?, data_source = ?, script_id = ?
        WHERE id = ?
    """, (
        schema_name if schema_name else '',
        schema_name,
        table_name,
        table_type,
        table_cn_name,
        new_data_source or table_data.get('data_source', ''),
        current_script_id,
        table_id
    ))

    # åˆ é™¤ç°æœ‰å­—æ®µï¼Œé‡æ–°æ’å…¥
    cursor.execute("DELETE FROM columns WHERE table_id = ?", (table_id,))

    # æ’å…¥æ–°å­—æ®µ
    _insert_columns(cursor, table_id, schema_name, table_name, table_data['columns'], script_id)


def _supplement_chinese_names_only(cursor: sqlite3.Cursor, table_id: str, table_data: Dict, script_id: str = None):
    """
    åªè¡¥å……ä¸­æ–‡åä¿¡æ¯ï¼Œä¸æ£€æŸ¥å­—æ®µå­˜åœ¨æ€§

    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        table_id: è¡¨ID
        table_data: æ–°çš„è¡¨æ•°æ®
        script_id: è„šæœ¬ID
    """
    # è¯»å–ç°æœ‰å­—æ®µ
    cursor.execute("""
        SELECT column_name, description
        FROM columns
        WHERE table_id = ?
    """, (table_id,))

    existing_columns = {row['column_name']: row['description'] for row in cursor.fetchall()}

    # åªè¡¥å……ä¸­æ–‡å
    for col in table_data['columns']:
        col_en_nm = col.get('col_en_nm')
        col_cn_nm = col.get('col_cn_nm')

        if col_en_nm in existing_columns and col_cn_nm and not existing_columns[col_en_nm]:
            cursor.execute("""
                UPDATE columns
                SET description = ?
                WHERE table_id = ? AND column_name = ?
            """, (col_cn_nm, table_id, col_en_nm))
            print(f"    â• è¡¥å……å­—æ®µä¸­æ–‡å: {col_en_nm} -> {col_cn_nm}")


def _merge_statement_info(cursor: sqlite3.Cursor, table_id: str, table_data: Dict, script_id: str = None,
                         existing_data_source: str = None, new_data_source: str = None):
    """
    åˆå¹¶è¯­å¥ä¿¡æ¯ï¼Œå…è®¸æ–°å¢å­—æ®µ

    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        table_id: è¡¨ID
        table_data: æ–°çš„è¡¨æ•°æ®
        script_id: è„šæœ¬ID
        existing_data_source: ç°æœ‰æ•°æ®æºç±»å‹
        new_data_source: æ–°æ•°æ®æºç±»å‹
    """
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']

    # è¯»å–ç°æœ‰å­—æ®µ
    cursor.execute("""
        SELECT column_name, data_type, is_nullable, default_value,
               is_primary_key, is_foreign_key, description, ordinal_position
        FROM columns
        WHERE table_id = ?
    """, (table_id,))

    existing_columns = {row['column_name']: dict(row) for row in cursor.fetchall()}

    # å¤„ç†æ–°å­—æ®µ
    for col in table_data['columns']:
        col_en_nm = col.get('col_en_nm')

        if col_en_nm in existing_columns:
            # å­—æ®µå·²å­˜åœ¨ï¼Œåˆå¹¶ä¿¡æ¯
            existing = existing_columns[col_en_nm]
            col_cn_nm = col.get('col_cn_nm')

            # æ£€æŸ¥ä¸­æ–‡åå†²çª
            if col_cn_nm and existing['description'] and col_cn_nm != existing['description']:
                print(f"    âš ï¸ å­—æ®µä¸­æ–‡åå†²çª: {schema_name}.{table_name}.{col_en_nm}")
                print(f"       ç°æœ‰: '{existing['description']}', æ–°: '{col_cn_nm}'")
                # ä¿ç•™ç°æœ‰ä¸­æ–‡åï¼ˆæŒ‰æ—¶é—´ä¼˜å…ˆï¼‰

            # è¡¥å……ç¼ºå¤±çš„ä¸­æ–‡å
            elif col_cn_nm and not existing['description']:
                cursor.execute("""
                    UPDATE columns
                    SET description = ?
                    WHERE table_id = ? AND column_name = ?
                """, (col_cn_nm, table_id, col_en_nm))
                print(f"    â• è¡¥å……å­—æ®µä¸­æ–‡å: {col_en_nm} -> {col_cn_nm}")

        else:
            # æ–°å­—æ®µï¼Œæ·»åŠ å®ƒ
            print(f"    â• æ–°å¢å­—æ®µ: {col_en_nm}")
            _insert_single_column(cursor, table_id, schema_name, table_name, col, script_id)


def _insert_single_column(cursor: sqlite3.Cursor, table_id: str, schema_name: str, table_name: str,
                         col: Dict, script_id: str = None):
    """æ’å…¥å•ä¸ªå­—æ®µ"""
    col_no = col.get('col_no', 1)
    col_en_nm = col.get('col_en_nm', '')
    col_cn_nm = col.get('col_cn_nm', '')
    data_type = col.get('data_type', '')
    is_null = col.get('is_null', True)
    default_value = col.get('default_value', '')
    is_pri_key = col.get('is_pri_key', False)
    is_foreign_key = col.get('is_foreign_key', False)

    # ç”Ÿæˆå­—æ®µID
    column_id = _generate_column_id(schema_name, table_name, col_en_nm, script_id if script_id else None)

    cursor.execute("""
        INSERT INTO columns (
            id, table_id, column_name, data_type, max_length, is_nullable,
            default_value, is_primary_key, is_foreign_key, description, ordinal_position
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        column_id,
        table_id,
        col_en_nm,
        data_type,
        None,  # max_length
        is_null,
        default_value,
        is_pri_key,
        is_foreign_key,
        col_cn_nm,
        col_no
    ))


def _insert_new_table(cursor: sqlite3.Cursor, table_data: Dict, script_id: str = None):
    """æ’å…¥æ–°è¡¨"""
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    table_cn_name = table_data['table_cn_name']
    data_source = table_data['data_source']
    table_type = table_data['table_type']
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    is_tmp_table = (table_type == 'TMP_TABLE')
    current_script_id = script_id if is_tmp_table else None

    # ç”Ÿæˆè¡¨IDï¼ˆä¸´æ—¶è¡¨éœ€è¦ä¼ å…¥script_idï¼‰
    table_id = _generate_table_id(schema_name, table_name, current_script_id)
    
    # å¤„ç†database_id
    database_id = ''
    if schema_name:
        # ç¡®ä¿databaseè®°å½•å­˜åœ¨
        cursor.execute("SELECT id FROM databases WHERE id = ?", (schema_name,))
        if not cursor.fetchone():
            cursor.execute("""
                INSERT INTO databases (id, name, description)
                VALUES (?, ?, '')
            """, (schema_name, schema_name))
            print(f"  ğŸ“‚ åˆ›å»ºæ•°æ®åº“è®°å½•: {schema_name}")
        database_id = schema_name
    
    # æ’å…¥è¡¨è®°å½•ï¼ˆæ–‡æœ¬å­—æ®µä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä»£æ›¿NULLï¼Œæ•°å€¼/æ—¥æœŸå­—æ®µä½¿ç”¨NULLï¼‰
    cursor.execute("""
        INSERT INTO tables (
            id, database_id, schema_name, table_name, table_type,
            description, business_purpose, data_source, refresh_frequency,
            row_count, data_size_mb, last_updated, script_id
        ) VALUES (?, ?, ?, ?, ?, ?, '', ?, 'DAILY', NULL, NULL, NULL, ?)
    """, (
        table_id,
        database_id,
        schema_name or '',
        table_name,
        table_type,
        table_cn_name or '',
        data_source,
        current_script_id or ''  # å°†Noneè½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
    ))
    
    print(f"  âœ… æ’å…¥è¡¨: {table_id} (ç±»å‹: {table_type})")
    
    # æ’å…¥å­—æ®µè®°å½•
    _insert_columns(cursor, table_id, schema_name, table_name, table_data['columns'], current_script_id)


def _update_table_with_ddl(cursor: sqlite3.Cursor, table_id: str, table_data: Dict, script_id: str = None):
    """ç”¨DDLè¦†ç›–DMLè¡¨"""
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    table_cn_name = table_data['table_cn_name']
    table_type = table_data['table_type']
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    is_tmp_table = (table_type == 'TMP_TABLE')
    current_script_id = script_id if is_tmp_table else None
    
    # è¯»å–ç°æœ‰å­—æ®µçš„ä¸­æ–‡åï¼ˆDMLå¯èƒ½æœ‰ï¼‰
    cursor.execute("""
        SELECT column_name, description
        FROM columns
        WHERE table_id = ? AND description IS NOT NULL AND description != ''
    """, (table_id,))
    
    existing_col_descriptions = {row['column_name']: row['description'] for row in cursor.fetchall()}
    
    # è·å–å®é™…çš„data_sourceï¼ˆä»table_dataä¸­ï¼‰
    actual_data_source = table_data.get('data_source', 'CREATE_TABLE')
    
    # æ›´æ–°è¡¨ä¿¡æ¯ï¼ˆæ–‡æœ¬å­—æ®µä½¿ç”¨ç©ºå­—ç¬¦ä¸²ï¼‰
    cursor.execute("""
        UPDATE tables
        SET table_type = ?,
            description = ?,
            data_source = ?
        WHERE id = ?
    """, (table_type, table_cn_name or '', actual_data_source, table_id))
    
    print(f"  âœ… æ›´æ–°è¡¨ä¿¡æ¯ï¼Œdata_source={actual_data_source}")
    
    # åˆ é™¤æ—§å­—æ®µ
    cursor.execute("DELETE FROM columns WHERE table_id = ?", (table_id,))
    
    # æ’å…¥DDLå­—æ®µï¼Œè¡¥å……DMLçš„ä¸­æ–‡å
    for col in table_data['columns']:
        col_en_nm = col.get('col_en_nm')
        if col_en_nm in existing_col_descriptions:
            # å¦‚æœDDLæ²¡æœ‰ä¸­æ–‡åï¼Œä½¿ç”¨DMLçš„
            if not col.get('col_cn_nm'):
                col['col_cn_nm'] = existing_col_descriptions[col_en_nm]
    
    _insert_columns(cursor, table_id, schema_name, table_name, table_data['columns'], current_script_id)


def _supplement_ddl_with_dml(cursor: sqlite3.Cursor, table_id: str, table_data: Dict, script_id: str = None):
    """è¡¥å……DMLä¿¡æ¯åˆ°DDLè¡¨"""
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    
    # è¯»å–ç°æœ‰å­—æ®µ
    cursor.execute("""
        SELECT column_name, description
        FROM columns
        WHERE table_id = ?
    """, (table_id,))
    
    existing_columns = {row['column_name']: row['description'] for row in cursor.fetchall()}
    
    # æ£€æŸ¥DMLçš„å­—æ®µ
    for col in table_data['columns']:
        col_en_nm = col.get('col_en_nm')
        
        if col_en_nm not in existing_columns:
            # DMLæœ‰æ–°å­—æ®µï¼ŒDDLæ²¡æœ‰ - æŠ¥é”™
            raise Exception(
                f"DMLè¯­å¥å¼•ç”¨äº†DDLä¸­ä¸å­˜åœ¨çš„å­—æ®µ: "
                f"{schema_name}.{table_name}.{col_en_nm}"
            )
        
        # è¡¥å……ä¸­æ–‡å
        col_cn_nm = col.get('col_cn_nm')
        if col_cn_nm and not existing_columns[col_en_nm]:
            cursor.execute("""
                UPDATE columns
                SET description = ?
                WHERE table_id = ? AND column_name = ?
            """, (col_cn_nm, table_id, col_en_nm))
            print(f"    â• è¡¥å……å­—æ®µä¸­æ–‡å: {col_en_nm} -> {col_cn_nm}")


def _merge_dml_with_dml(cursor: sqlite3.Cursor, table_id: str, table_data: Dict, script_id: str = None):
    """åˆå¹¶DMLä¸DML"""
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    table_type = table_data['table_type']
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    is_tmp_table = (table_type == 'TMP_TABLE')
    current_script_id = script_id if is_tmp_table else None
    
    # è¯»å–ç°æœ‰å­—æ®µ
    cursor.execute("""
        SELECT column_name, data_type, is_nullable, default_value,
               is_primary_key, is_foreign_key, description, ordinal_position
        FROM columns
        WHERE table_id = ?
    """, (table_id,))
    
    existing_columns = {row['column_name']: dict(row) for row in cursor.fetchall()}
    
    # å¤„ç†æ–°å­—æ®µ
    for col in table_data['columns']:
        col_en_nm = col.get('col_en_nm')
        
        if col_en_nm in existing_columns:
            # å­—æ®µå·²å­˜åœ¨ï¼Œæ£€æŸ¥å†²çª
            existing = existing_columns[col_en_nm]
            col_cn_nm = col.get('col_cn_nm')
            
            # æ£€æŸ¥å†²çªï¼šå¦‚æœä¸¤ä¸ªéƒ½æœ‰å€¼ä¸”ä¸åŒï¼Œåˆ™æŠ¥é”™
            if col_cn_nm and existing['description'] and col_cn_nm != existing['description']:
                raise Exception(
                    f"å­—æ®µä¸­æ–‡åå†²çª: {schema_name}.{table_name}.{col_en_nm} "
                    f"ç°æœ‰: '{existing['description']}', æ–°: '{col_cn_nm}'"
                )
            
            # ç”¨æœ‰å€¼è¦†ç›–æ— å€¼
            if col_cn_nm and not existing['description']:
                cursor.execute("""
                    UPDATE columns
                    SET description = ?
                    WHERE table_id = ? AND column_name = ?
                """, (col_cn_nm, table_id, col_en_nm))
                print(f"    ğŸ”„ æ›´æ–°å­—æ®µä¸­æ–‡å: {col_en_nm} -> {col_cn_nm}")
        
        else:
            # æ–°å­—æ®µï¼Œç›´æ¥æ·»åŠ ï¼ˆéœ€è¦ä¼ å…¥script_idä»¥æ”¯æŒä¸´æ—¶è¡¨ï¼‰
            col_id = _generate_column_id(schema_name, table_name, col_en_nm, current_script_id)
            cursor.execute("""
                INSERT INTO columns (
                    id, table_id, column_name, data_type, max_length,
                    is_nullable, default_value, is_primary_key, is_foreign_key,
                    description, ordinal_position
                ) VALUES (?, ?, ?, '', NULL, 1, '', 0, 0, ?, NULL)
            """, (
                col_id,
                table_id,
                col_en_nm,
                col.get('col_cn_nm') or ''
            ))
            print(f"    âœ¨ æ·»åŠ æ–°å­—æ®µ: {col_en_nm}")


def _insert_columns(cursor: sqlite3.Cursor, table_id: str, schema_name: str,
                   table_name: str, columns: List[Dict], script_id: str = None):
    """æ’å…¥å­—æ®µè®°å½•"""
    for col in columns:
        col_en_nm = col.get('col_en_nm')
        if not col_en_nm:
            continue

        col_id = _generate_column_id(schema_name, table_name, col_en_nm, script_id)
        
        # ä»colå­—å…¸ä¸­è·å–å€¼ï¼Œè®¾ç½®é»˜è®¤å€¼ï¼ˆæ–‡æœ¬å­—æ®µä½¿ç”¨ç©ºå­—ç¬¦ä¸²ï¼Œæ•°å€¼å­—æ®µä½¿ç”¨Noneï¼‰
        data_type = col.get('data_type') or ''
        max_length = None  # æš‚ä¸å¤„ç†ï¼Œæ•°å€¼å­—æ®µ
        is_nullable = 0 if col.get('is_null') == False else 1
        default_value = col.get('default_value') or ''
        is_primary_key = 1 if col.get('is_pri_key') else 0
        is_foreign_key = 1 if col.get('is_foreign_key') else 0
        description = col.get('col_cn_nm') or ''
        ordinal_position = col.get('col_no') or None  # æ•°å€¼å­—æ®µ
        
        cursor.execute("""
            INSERT INTO columns (
                id, table_id, column_name, data_type, max_length,
                is_nullable, default_value, is_primary_key, is_foreign_key,
                description, ordinal_position
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            col_id, table_id, col_en_nm, data_type, max_length,
            is_nullable, default_value, is_primary_key, is_foreign_key,
            description, ordinal_position
        ))
    
    print(f"  âœ… æ’å…¥ {len(columns)} ä¸ªå­—æ®µ")


def _build_dependency_graph(extracted_data: List[Dict]) -> nx.DiGraph:
    """
    æ„å»ºSQLä¾èµ–å›¾
    
    Args:
        extracted_data: æå–çš„å…ƒæ•°æ®åˆ—è¡¨
    
    Returns:
        NetworkXæœ‰å‘å›¾
    """
    graph = nx.DiGraph()
    
    for metadata in extracted_data:
        target_table = metadata['target_table']
        target_schema = target_table.get('schema_nm', '') or ''
        target_name = target_table.get('tbl_en_nm', '')
        
        if not target_name:
            continue
        
        # æ„é€ å®Œæ•´çš„è¡¨æ ‡è¯†
        target_full_name = f"{target_schema}.{target_name}" if target_schema else target_name
        
        # æ·»åŠ ç›®æ ‡è¡¨èŠ‚ç‚¹
        if target_full_name not in graph:
            graph.add_node(target_full_name, schema=target_schema, table=target_name)
        
        # æ·»åŠ æ¥æºè¡¨å’Œè¾¹ï¼ˆDMLæˆ–CREATE ASï¼‰
        # CREATE ASè¯­å¥ä¹Ÿæœ‰æ¥æºè¡¨ä¾èµ–
        if 'source_tables' in metadata and metadata['source_tables']:
            for source_table in metadata['source_tables']:
                source_schema = source_table.get('schema_nm', '') or ''
                source_name = source_table.get('tbl_en_nm', '')
                
                if not source_name:
                    continue
                
                # æ„é€ å®Œæ•´çš„æ¥æºè¡¨æ ‡è¯†
                source_full_name = f"{source_schema}.{source_name}" if source_schema else source_name
                
                # æ·»åŠ æ¥æºè¡¨èŠ‚ç‚¹
                if source_full_name not in graph:
                    graph.add_node(source_full_name, schema=source_schema, table=source_name)
                
                # æ·»åŠ è¾¹ï¼ˆæ¥æºè¡¨ -> ç›®æ ‡è¡¨ï¼‰ï¼Œé¿å…é‡å¤
                if not graph.has_edge(source_full_name, target_full_name):
                    graph.add_edge(source_full_name, target_full_name)
    
    return graph


def _save_dependency_graph(sql_file_path: str, graph: nx.DiGraph) -> str:
    """
    ä¿å­˜ä¾èµ–å›¾åˆ°JSONæ–‡ä»¶
    
    Args:
        sql_file_path: SQLæ–‡ä»¶è·¯å¾„
        graph: ä¾èµ–å›¾
    
    Returns:
        JSONæ–‡ä»¶è·¯å¾„
    """
    # ç”ŸæˆJSONæ–‡ä»¶å
    base_name = os.path.splitext(os.path.basename(sql_file_path))[0]
    dir_name = os.path.dirname(sql_file_path)
    if not dir_name:
        dir_name = '.'
    json_file_path = os.path.join(dir_name, f"{base_name}_graph.json")
    
    # ä½¿ç”¨networkxçš„node-linkæ ¼å¼å¯¼å‡º
    graph_data = nx.node_link_data(graph)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(json_file_path, 'w', encoding='utf-8') as f:
        json.dump(graph_data, f, ensure_ascii=False, indent=2)
    
    return json_file_path


def _identify_target_and_source_tables(
    graph: nx.DiGraph,
    extracted_data: List[Dict] = None
) -> Tuple[Set[str], Set[str]]:
    """
    è¯†åˆ«ç›®æ ‡è¡¨å’Œæ¥æºè¡¨
    
    ç›®æ ‡è¡¨è¯†åˆ«é€»è¾‘ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
    1. å…¥åº¦>0çš„éä¸´æ—¶è¡¨ï¼ˆæœ‰æ•°æ®æµå…¥çš„å®ä½“è¡¨ï¼‰
    2. å¦‚æœ1æœªæ‰¾åˆ°ï¼Œåˆ™æ‰¾å‡ºåº¦=0çš„è¡¨ï¼ˆæœ€ç»ˆèŠ‚ç‚¹ï¼‰
    3. å¦‚æœä»æœªæ‰¾åˆ°ï¼Œè¿”å›ç©ºé›†åˆ
    
    æ¥æºè¡¨: å…¥åº¦ä¸º0çš„è¡¨
    
    Args:
        graph: ä¾èµ–å›¾
        extracted_data: æå–çš„å…ƒæ•°æ®åˆ—è¡¨
    
    Returns:
        (ç›®æ ‡è¡¨é›†åˆ, æ¥æºè¡¨é›†åˆ)
    """
    target_tables = set()
    source_tables = set()
    
    # è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    def is_temp_table(table_name: str) -> bool:
        """
        ä¸´æ—¶è¡¨åˆ¤æ–­è§„åˆ™ï¼š
        1. æ²¡æœ‰schemaï¼ˆä¸åŒ…å«'.'ï¼‰
        2. æˆ–è€…ä»¥å¸¸è§ä¸´æ—¶è¡¨å‰ç¼€å¼€å¤´ï¼ˆVT_ã€TMP_ã€TEMP_ç­‰ï¼‰
        """
        if '.' not in table_name:
            # æ²¡æœ‰schemaçš„è¡¨ï¼Œå¯èƒ½æ˜¯ä¸´æ—¶è¡¨
            # è¿›ä¸€æ­¥æ£€æŸ¥è¡¨åç‰¹å¾
            table_only = table_name.upper()
            temp_prefixes = ['VT_', 'TMP_', 'TEMP_', 'VOLATILE_', '#']
            return any(table_only.startswith(prefix) for prefix in temp_prefixes)
        return False
    
    # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„åº¦æ•°ä¿¡æ¯
    nodes_info = []
    for node in graph.nodes():
        out_degree = graph.out_degree(node)
        in_degree = graph.in_degree(node)
        is_temp = is_temp_table(node)
        nodes_info.append({
            'node': node,
            'in_degree': in_degree,
            'out_degree': out_degree,
            'is_temp': is_temp
        })
    
    # ç­–ç•¥1ï¼šæ‰¾å…¥åº¦>0çš„éä¸´æ—¶è¡¨
    for info in nodes_info:
        if info['in_degree'] > 0 and not info['is_temp']:
            target_tables.add(info['node'])
    
    # ç­–ç•¥2ï¼šå¦‚æœç­–ç•¥1æœªæ‰¾åˆ°ï¼Œæ‰¾å‡ºåº¦=0çš„è¡¨
    if not target_tables:
        for info in nodes_info:
            if info['out_degree'] == 0:
                target_tables.add(info['node'])
    
    # è¯†åˆ«æ¥æºè¡¨ï¼ˆå…¥åº¦=0ï¼‰
    for info in nodes_info:
        if info['in_degree'] == 0:
            source_tables.add(info['node'])
    
    return target_tables, source_tables


def _create_external_table_record(cursor: sqlite3.Cursor, schema_name: str, table_name: str):
    """
    åˆ›å»ºå¤–éƒ¨è¡¨çš„åŸºç¡€è®°å½•
    
    å¤–éƒ¨è¡¨æ˜¯æŒ‡åœ¨å½“å‰è„šæœ¬ä¸­è¢«å¼•ç”¨ä½†æœªå®šä¹‰çš„è¡¨ï¼ˆé€šå¸¸æ˜¯å…¶ä»–ç³»ç»Ÿçš„è¡¨ï¼‰
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        schema_name: Schemaåç§°
        table_name: è¡¨å
    """
    table_id = _generate_table_id(schema_name, table_name, None)
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
    cursor.execute("SELECT id FROM tables WHERE id = ?", (table_id,))
    if cursor.fetchone():
        return  # å·²å­˜åœ¨ï¼Œä¸é‡å¤åˆ›å»º
    
    # è·å–æˆ–åˆ›å»ºdatabase_id
    database_id = schema_name if schema_name else ''
    if schema_name:
        cursor.execute("SELECT id FROM databases WHERE id = ?", (database_id,))
        if not cursor.fetchone():
            cursor.execute("""
                INSERT INTO databases (id, name, description) 
                VALUES (?, ?, '')
            """, (database_id, schema_name))
    
    # æ’å…¥å¤–éƒ¨è¡¨è®°å½•
    cursor.execute("""
        INSERT INTO tables (
            id, database_id, schema_name, table_name, table_type,
            description, business_purpose, data_source,
            refresh_frequency, row_count, data_size_mb, script_id
        ) VALUES (?, ?, ?, ?, ?, '', 'å¤–éƒ¨è¡¨ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰', '', 'EXTERNAL', '', NULL, NULL, '')
    """, (
        table_id,
        database_id,
        schema_name,
        table_name,
        'TABLE'  # å¤–éƒ¨è¡¨é»˜è®¤ä¸ºTABLEç±»å‹
    ))


def _cleanup_script_data(cursor: sqlite3.Cursor, script_id: str) -> None:
    """
    æ¸…ç†è„šæœ¬çš„æ—§æ•°æ®ï¼ˆä¸ºå¢é‡æ›´æ–°åšå‡†å¤‡ï¼‰
    
    åœ¨é‡æ–°å¤„ç†è„šæœ¬å‰ï¼Œåˆ é™¤è¯¥è„šæœ¬çš„æ‰€æœ‰ç›¸å…³æ•°æ®ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        script_id: è„šæœ¬ID
    """
    # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
    cursor.execute("SELECT id FROM sql_scripts WHERE id = ?", (script_id,))
    if not cursor.fetchone():
        # è„šæœ¬ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†
        return
    
    deleted_counts = {}
    
    # 1. åˆ é™¤summaryï¼ˆå¿…é¡»å…ˆåˆ é™¤ï¼Œå› ä¸ºä¾èµ–detailï¼‰
    cursor.execute("DELETE FROM data_lineage_summary WHERE script_id = ?", (script_id,))
    deleted_counts['summary'] = cursor.rowcount
    
    # 2. åˆ é™¤detail
    cursor.execute("DELETE FROM data_lineage_detail WHERE script_id = ?", (script_id,))
    deleted_counts['detail'] = cursor.rowcount
    
    # 3. åˆ é™¤statements
    cursor.execute("DELETE FROM script_statements WHERE script_id = ?", (script_id,))
    deleted_counts['statements'] = cursor.rowcount
    
    # æ³¨æ„ï¼šä¸åˆ é™¤ä¸´æ—¶è¡¨ï¼Œå› ä¸ºï¼š
    # 1. ä¸´æ—¶è¡¨å¯èƒ½è¢«å…¶ä»–è„šæœ¬å¼•ç”¨ï¼ˆè™½ç„¶ä¸å¸¸è§ï¼‰
    # 2. ä¸´æ—¶è¡¨ä¼šåœ¨ä¸‹æ¬¡å¤„ç†æ—¶è‡ªåŠ¨æ›´æ–°
    # 3. å¦‚æœéœ€è¦æ¸…ç†ï¼Œå¯ä»¥æ‰‹åŠ¨å¤„ç†
    
    if any(deleted_counts.values()):
        print(f"  ğŸ§¹ æ¸…ç†æ—§æ•°æ®: summary={deleted_counts['summary']}, "
              f"detail={deleted_counts['detail']}, "
              f"statements={deleted_counts['statements']}")


def _populate_script_tables(
    cursor: sqlite3.Cursor,
    sql_file_path: str,
    sql_content: str,
    target_tables: Set[str],
    source_tables: Set[str],
    extracted_data: List[Dict],
    dependency_graph: nx.DiGraph,
    parsed_statements: List[exp.Expression],
    script_id: str
):
    """
    å¡«å……sql_scriptsã€script_statementsã€data_lineage_detailè¡¨
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        sql_file_path: SQLæ–‡ä»¶è·¯å¾„
        sql_content: SQLæ–‡ä»¶å†…å®¹
        target_tables: ç›®æ ‡è¡¨é›†åˆï¼ˆå®Œæ•´åç§°ï¼Œå¦‚schema.tableï¼‰
        source_tables: æ¥æºè¡¨é›†åˆ
        extracted_data: æå–çš„å…ƒæ•°æ®ï¼ˆæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€æ¡è¯­å¥ï¼‰
        dependency_graph: ä¾èµ–å›¾
        parsed_statements: è§£æåçš„SQLè¯­å¥åˆ—è¡¨
        script_id: è„šæœ¬ID
    """
    # ç”Ÿæˆscript_nameï¼ˆåªä½¿ç”¨è„šæœ¬åï¼Œä¸å«æ‰©å±•åï¼‰
    script_name = os.path.splitext(os.path.basename(sql_file_path))[0]
    
    # 0. æ¸…ç†æ—§æ•°æ®ï¼ˆå¦‚æœè„šæœ¬å·²å­˜åœ¨ï¼Œæ”¯æŒå¢é‡æ›´æ–°ï¼‰
    _cleanup_script_data(cursor, script_id)
    
    # 1. æ’å…¥sql_scriptsè¡¨ï¼ˆä¸€ä¸ªè„šæœ¬åªæœ‰ä¸€æ¡è®°å½•ï¼‰
    cursor.execute("""
        INSERT OR REPLACE INTO sql_scripts (
            id, script_name, script_content,
            script_type, script_purpose, author, description,
            execution_frequency, execution_order, is_active,
            last_executed, avg_execution_time_seconds, performance_stats_json
        ) VALUES (?, ?, ?, '', '', '', '', 'DAILY', NULL, 1, NULL, NULL, NULL)
    """, (
        script_id,
        script_name,
        sql_content
    ))
    
    # 2. å¡«å……script_statementsè¡¨ï¼ˆæŒ‰è¯­å¥ï¼‰
    print(f"  ğŸ“ å¡«å……script_statementsè¡¨...")
    for idx, parsed_sql in enumerate(parsed_statements, 1):
        if parsed_sql is None:
            continue
        
        statement_id = f"{script_id}__STMT_{idx:03d}"
        statement_type = _classify_statement_type(parsed_sql)
        statement_content = parsed_sql.sql()
        
        # æå–è¯¥è¯­å¥çš„ç›®æ ‡è¡¨
        target_table_id = None
        if idx <= len(extracted_data):
            metadata = extracted_data[idx - 1]
            target_table = metadata.get('target_table', {})
            target_schema = target_table.get('schema_nm', '') or ''
            target_name = target_table.get('tbl_en_nm', '')
            
            if target_name:
                # å°è¯•å®ä½“è¡¨
                target_table_id = _generate_table_id(target_schema, target_name, None)
                cursor.execute("SELECT id FROM tables WHERE id = ?", (target_table_id,))
                if not cursor.fetchone():
                    # å°è¯•ä¸´æ—¶è¡¨
                    target_table_id = _generate_table_id(target_schema, target_name, script_id)
                    cursor.execute("SELECT id FROM tables WHERE id = ?", (target_table_id,))
                    if not cursor.fetchone():
                        target_table_id = None
        
        # æ’å…¥statementè®°å½•
        cursor.execute("""
            INSERT OR REPLACE INTO script_statements (
                id, script_id, statement_index, statement_type,
                statement_content, target_table_id, description
            ) VALUES (?, ?, ?, ?, ?, ?, '')
        """, (
            statement_id,
            script_id,
            idx,
            statement_type,
            statement_content,
            target_table_id
        ))
    
    print(f"  âœ… å·²å¡«å…… {len([s for s in parsed_statements if s is not None])} æ¡è¯­å¥è®°å½•")
    
    # 3. å¡«å……data_lineage_detailè¡¨ï¼ˆæŒ‰è¯­å¥ï¼‰
    print(f"  ğŸ“Š å¡«å……data_lineage_detailè¡¨...")
    lineage_count = 0
    
    for idx, metadata in enumerate(extracted_data, 1):
        statement_id = f"{script_id}__STMT_{idx:03d}"
        
        # è·å–è¯¥è¯­å¥çš„ç›®æ ‡è¡¨
        target_table = metadata.get('target_table', {})
        target_schema = target_table.get('schema_nm', '') or ''
        target_name = target_table.get('tbl_en_nm', '')
        
        if not target_name:
            continue
        
        # æŸ¥æ‰¾ç›®æ ‡è¡¨IDï¼ˆå…ˆå®ä½“è¡¨ï¼Œå†ä¸´æ—¶è¡¨ï¼‰
        target_table_id = _generate_table_id(target_schema, target_name, None)
        cursor.execute("SELECT id FROM tables WHERE id = ?", (target_table_id,))
        if not cursor.fetchone():
            target_table_id = _generate_table_id(target_schema, target_name, script_id)
            cursor.execute("SELECT id FROM tables WHERE id = ?", (target_table_id,))
            if not cursor.fetchone():
                print(f"  âš ï¸  è¯­å¥{idx}çš„ç›®æ ‡è¡¨ {target_schema}.{target_name} ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œè·³è¿‡")
                continue
        
        # è·å–è¯¥è¯­å¥çš„æ¥æºè¡¨
        source_tables_in_stmt = metadata.get('source_tables', [])
        
        for source_table in source_tables_in_stmt:
            source_schema = source_table.get('schema_nm', '') or ''
            source_name = source_table.get('tbl_en_nm', '')
            
            if not source_name:
                continue
            
            # æŸ¥æ‰¾æ¥æºè¡¨IDï¼ˆå…ˆå®ä½“è¡¨ï¼Œå†ä¸´æ—¶è¡¨ï¼‰
            source_table_id = _generate_table_id(source_schema, source_name, None)
            cursor.execute("SELECT id FROM tables WHERE id = ?", (source_table_id,))
            if not cursor.fetchone():
                source_table_id = _generate_table_id(source_schema, source_name, script_id)
                cursor.execute("SELECT id FROM tables WHERE id = ?", (source_table_id,))
                if not cursor.fetchone():
                    # æ¥æºè¡¨ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»ºå¤–éƒ¨è¡¨è®°å½•
                    print(f"  ğŸ“¥ è‡ªåŠ¨åˆ›å»ºå¤–éƒ¨è¡¨è®°å½•: {source_schema}.{source_name}")
                    _create_external_table_record(cursor, source_schema, source_name)
                    source_table_id = _generate_table_id(source_schema, source_name, None)
            
            # ç”Ÿæˆlineage_id
            lineage_id = f"{target_table_id}__{source_table_id}__{statement_id}"
            
            # æ’å…¥data_lineage_detail
            cursor.execute("""
                INSERT OR REPLACE INTO data_lineage_detail (
                    id, target_table_id, source_table_id, script_id, statement_id,
                    transformation_logic, filter_conditions
                ) VALUES (?, ?, ?, ?, ?, '', '')
            """, (
                lineage_id,
                target_table_id,
                source_table_id,
                script_id,
                statement_id
            ))
            lineage_count += 1
    
    print(f"  âœ… å·²å¡«å…… {lineage_count} æ¡è¡€ç¼˜è®°å½•")
    
    # 4. ç”Ÿæˆdata_lineage_summaryï¼ˆä»detailæ¨å¯¼ï¼‰
    print(f"  ğŸ”„ æ­£åœ¨ç”Ÿæˆsummary...")
    try:
        from lineage_graph_manager import generate_lineage_summary
        generate_lineage_summary(cursor, script_id)
    except Exception as e:
        print(f"  âš ï¸  Summaryç”Ÿæˆå¤±è´¥: {e}")
        # ä¸å½±å“ä¸»æµç¨‹


def _update_global_lineage(
    sql_file_path: str,
    target_tables: Set[str],
    source_tables: Set[str],
    lineage_json_path: str = 'datalineage.json'
):
    """
    æ›´æ–°å…¨å±€è¡€ç¼˜å›¾ï¼ˆdatalineage.jsonï¼‰
    
    Args:
        sql_file_path: SQLæ–‡ä»¶è·¯å¾„
        target_tables: ç›®æ ‡è¡¨é›†åˆï¼ˆå®Œæ•´åç§°ï¼‰
        source_tables: æ¥æºè¡¨é›†åˆ
        lineage_json_path: è¡€ç¼˜å›¾JSONæ–‡ä»¶è·¯å¾„
    """
    # è¯»å–ç°æœ‰è¡€ç¼˜å›¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(lineage_json_path):
        with open(lineage_json_path, 'r', encoding='utf-8') as f:
            graph_data = json.load(f)
        global_graph = nx.node_link_graph(graph_data, directed=True)
    else:
        global_graph = nx.DiGraph()
    
    # ä¸ºæ¯ä¸ªç›®æ ‡è¡¨å¤„ç†è¡€ç¼˜å…³ç³»
    for target_table in target_tables:
        # æ·»åŠ ç›®æ ‡è¡¨èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if target_table not in global_graph:
            target_schema, target_name = _parse_full_table_name(target_table)
            global_graph.add_node(target_table, schema=target_schema, table=target_name)
        
        # æ·»åŠ æ¥æºè¡¨èŠ‚ç‚¹å’Œè¾¹
        for source_table in source_tables:
            # æ·»åŠ æ¥æºè¡¨èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if source_table not in global_graph:
                source_schema, source_name = _parse_full_table_name(source_table)
                global_graph.add_node(source_table, schema=source_schema, table=source_name)
            
            # æ·»åŠ æˆ–æ›´æ–°è¾¹
            if global_graph.has_edge(source_table, target_table):
                # è¾¹å·²å­˜åœ¨ï¼Œæ›´æ–°script_pathså±æ€§
                edge_data = global_graph[source_table][target_table]
                script_paths = edge_data.get('script_paths', [])
                if sql_file_path not in script_paths:
                    script_paths.append(sql_file_path)
                    global_graph[source_table][target_table]['script_paths'] = script_paths
            else:
                # è¾¹ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è¾¹
                global_graph.add_edge(source_table, target_table, script_paths=[sql_file_path])
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    graph_data = nx.node_link_data(global_graph)
    with open(lineage_json_path, 'w', encoding='utf-8') as f:
        json.dump(graph_data, f, ensure_ascii=False, indent=2)


def _parse_full_table_name(full_name: str) -> Tuple[str, str]:
    """
    è§£æå®Œæ•´è¡¨åä¸º(schema, table)
    
    Args:
        full_name: å®Œæ•´è¡¨åï¼Œå¦‚"schema.table"æˆ–"table"
    
    Returns:
        (schema_name, table_name)
    """
    if '.' in full_name:
        parts = full_name.split('.')
        return parts[0], parts[1]
    else:
        return '', full_name


def process_sql_directory(
    directory_path: str,
    dialect: str = 'teradata',
    mode: str = 'insert',
    db_path: str = 'dw_metadata.db',
    lineage_json_path: str = 'datalineage.json',
    log_file: str = 'batch_process_log.txt'
) -> Dict:
    """
    æ‰¹é‡å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰SQLæ–‡ä»¶
    
    Args:
        directory_path: SQLæ–‡ä»¶ç›®å½•è·¯å¾„
        dialect: SQLæ–¹è¨€ï¼ˆå¦‚'mysql', 'teradata', 'postgres'ç­‰ï¼‰
        mode: å¤„ç†æ¨¡å¼
            - 'clear': æ¸…æ´—æ•°æ®åº“åå¤„ç†
            - 'insert': åœ¨å½“å‰æ•°æ®åŸºç¡€ä¸Šè¿½åŠ 
        db_path: SQLiteæ•°æ®åº“è·¯å¾„
        lineage_json_path: å…¨å±€è¡€ç¼˜å›¾JSONæ–‡ä»¶è·¯å¾„
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    
    Returns:
        {
            'success': True/False,
            'errors': [
                {'file': 'xxx.sql', 'error': 'xxx'}
            ]
        }
    """
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='w', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("="*70)
        logger.info("å¼€å§‹æ‰¹é‡å¤„ç†SQLæ–‡ä»¶")
        logger.info(f"ç›®å½•: {directory_path}")
        logger.info(f"æ–¹è¨€: {dialect}")
        logger.info(f"æ¨¡å¼: {mode}")
        logger.info("="*70)
        
        # 1. æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(directory_path):
            error_msg = f"ç›®å½•ä¸å­˜åœ¨: {directory_path}"
            logger.error(error_msg)
            return {'success': False, 'errors': [{'file': directory_path, 'error': error_msg}]}
        
        # 2. å¦‚æœæ˜¯clearæ¨¡å¼ï¼Œé‡æ–°åˆå§‹åŒ–æ•°æ®åº“
        if mode == 'clear':
            logger.info("\nğŸ”„ æ¨¡å¼: CLEAR - æ­£åœ¨é‡æ–°åˆå§‹åŒ–æ•°æ®åº“...")
            
            # åˆ é™¤æ—§çš„è¡€ç¼˜å›¾æ–‡ä»¶
            if os.path.exists(lineage_json_path):
                os.remove(lineage_json_path)
                logger.info(f"  å·²åˆ é™¤æ—§çš„è¡€ç¼˜å›¾æ–‡ä»¶: {lineage_json_path}")
            
            # è°ƒç”¨init_sqlite.pyé‡æ–°åˆå§‹åŒ–æ•°æ®åº“
            try:
                result = subprocess.run(
                    ['python', 'init_sqlite.py', '--force-reset'],
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    timeout=60
                )
                if result.returncode != 0:
                    error_msg = f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {result.stderr}"
                    logger.error(error_msg)
                    return {'success': False, 'errors': [{'file': 'init_sqlite.py', 'error': error_msg}]}
                logger.info("  âœ… æ•°æ®åº“å·²é‡æ–°åˆå§‹åŒ–")
            except subprocess.TimeoutExpired:
                error_msg = "æ•°æ®åº“åˆå§‹åŒ–è¶…æ—¶"
                logger.error(error_msg)
                return {'success': False, 'errors': [{'file': 'init_sqlite.py', 'error': error_msg}]}
            except Exception as e:
                error_msg = f"æ•°æ®åº“åˆå§‹åŒ–å¼‚å¸¸: {str(e)}"
                logger.error(error_msg)
                return {'success': False, 'errors': [{'file': 'init_sqlite.py', 'error': error_msg}]}
        
        # 3. é€’å½’æ‰«æç›®å½•ï¼Œè·å–æ‰€æœ‰SQLæ–‡ä»¶
        logger.info("\nğŸ“‚ æ­£åœ¨æ‰«æSQLæ–‡ä»¶...")
        sql_files = []
        for root, dirs, files in os.walk(directory_path):
            for file in files:
                if file.lower().endswith('.sql'):
                    sql_files.append(os.path.join(root, file))
        
        if not sql_files:
            logger.warning("  âš ï¸  æœªæ‰¾åˆ°ä»»ä½•SQLæ–‡ä»¶")
            return {'success': True, 'errors': []}
        
        logger.info(f"  âœ… æ‰¾åˆ° {len(sql_files)} ä¸ªSQLæ–‡ä»¶")
        
        # 4. é€ä¸ªå¤„ç†SQLæ–‡ä»¶
        logger.info("\nğŸ“Š å¼€å§‹å¤„ç†SQLæ–‡ä»¶...")
        errors = []
        
        for idx, sql_file in enumerate(sql_files, 1):
            relative_path = os.path.relpath(sql_file, directory_path)
            logger.info(f"\n[{idx}/{len(sql_files)}] å¤„ç†: {relative_path}")
            
            try:
                success, error_msg = process_sql_file(
                    sql_file_path=sql_file,
                    dialect=dialect,
                    db_path=db_path
                )
                
                if success:
                    logger.info(f"  âœ… æˆåŠŸ")
                else:
                    logger.error(f"  âŒ å¤±è´¥: {error_msg}")
                    errors.append({
                        'file': relative_path,
                        'error': error_msg
                    })
                    # ç«‹å³åœæ­¢æ¨¡å¼ï¼šé‡åˆ°é”™è¯¯ç›´æ¥è¿”å›
                    logger.error("\nâ›” é‡åˆ°é”™è¯¯ï¼Œåœæ­¢æ‰¹å¤„ç†")
                    return {'success': False, 'errors': errors}
                    
            except Exception as e:
                error_msg = f"å¤„ç†å¼‚å¸¸: {str(e)}"
                logger.error(f"  âŒ {error_msg}")
                errors.append({
                    'file': relative_path,
                    'error': error_msg
                })
                # ç«‹å³åœæ­¢æ¨¡å¼ï¼šé‡åˆ°é”™è¯¯ç›´æ¥è¿”å›
                logger.error("\nâ›” é‡åˆ°é”™è¯¯ï¼Œåœæ­¢æ‰¹å¤„ç†")
                return {'success': False, 'errors': errors}
        
        # 5. æ±‡æ€»ç»“æœ
        logger.info("\n" + "="*70)
        logger.info("æ‰¹å¤„ç†å®Œæˆ")
        logger.info(f"æ€»æ–‡ä»¶æ•°: {len(sql_files)}")
        logger.info(f"æˆåŠŸ: {len(sql_files) - len(errors)}")
        logger.info(f"å¤±è´¥: {len(errors)}")
        logger.info("="*70)
        
        if errors:
            logger.info("\nå¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
            for err in errors:
                logger.info(f"  âŒ {err['file']}")
                logger.info(f"     é”™è¯¯: {err['error']}")
        
        return {
            'success': len(errors) == 0,
            'errors': errors
        }
        
    except Exception as e:
        error_msg = f"æ‰¹å¤„ç†è¿‡ç¨‹å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}"
        logger.error(error_msg)
        return {'success': False, 'errors': [{'file': 'batch_process', 'error': error_msg}]}


# ä¸»ç¨‹åºå…¥å£ï¼ˆç”¨äºæµ‹è¯•ï¼‰
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        # print("ç”¨æ³•: python sql_file_processor.py <sql_file_path> [dialect]")
        # print("ç¤ºä¾‹: python sql_file_processor.py test.sql mysql")
        # sys.exit(1)
        
        sql_file = "C:\\pyworks\\Datasets\\SQLs\\DML\\Teradata\\minsheng\\MDB_TD\\sqls\\dm88_op_cnt_camp_ac_cs_ex_situ_mdm_10200.pl.1609.sql"
        sql_file = "C:\\pyworks\\Datasets\\SQLs\\DML\\Teradata\\minsheng\\MDB_TD\\sqls\\dm88_op_cnt_camp_ac_stat_trace_mdm_10200.pl.1615.sql"
        dialect = "teradata"

    else:
        sql_file = sys.argv[1]
        dialect = sys.argv[2] if len(sys.argv) > 2 else None
    
    success, error_msg = process_sql_file(sql_file, dialect=dialect)
    
    if success:
        print("\nğŸ‰ å¤„ç†æˆåŠŸï¼")
        sys.exit(0)
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {error_msg}")
        sys.exit(1)

