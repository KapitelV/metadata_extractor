"""
SQLæ–‡ä»¶å¤„ç†å™¨ - ä»SQLæ–‡ä»¶ä¸­æå–å…ƒæ•°æ®å¹¶å­˜å‚¨åˆ°SQLiteæ•°æ®åº“

åŠŸèƒ½ï¼š
1. è¯»å–SQLæ–‡ä»¶å†…å®¹
2. è§£æSQLè¯­å¥ï¼ˆDDLå’ŒDMLï¼‰
3. æå–è¡¨å’Œå­—æ®µå…ƒæ•°æ®
4. å¤„ç†ä¸ç°æœ‰æ•°æ®çš„å†²çª
5. å­˜å‚¨åˆ°dw_metadata.dbæ•°æ®åº“

ä½œè€…ï¼šAI Assistant
åˆ›å»ºæ—¶é—´ï¼š2025-01-23
"""

import sqlite3
import os
import json
import subprocess
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Set
import sqlglot
from sqlglot import exp
import networkx as nx
from metadata_extractor import extract_ddl_metadata, extract_sql_metadata


def process_sql_file(
    sql_file_path: str,
    dialect: str = None,
    db_path: str = 'dw_metadata.db'
) -> Tuple[bool, str]:
    """
    å¤„ç†SQLæ–‡ä»¶å¹¶å­˜å‚¨å…ƒæ•°æ®åˆ°æ•°æ®åº“
    
    Args:
        sql_file_path: SQLæ–‡ä»¶è·¯å¾„
        dialect: SQLæ–¹è¨€ï¼ˆå¦‚'mysql', 'teradata', 'postgres'ç­‰ï¼‰
        db_path: SQLiteæ•°æ®åº“è·¯å¾„ï¼Œé»˜è®¤ä¸º'dw_metadata.db'
    
    Returns:
        (True, '') - æˆåŠŸ
        (False, 'é”™è¯¯åŸå› ') - å¤±è´¥
    """
    try:
        # 1. è¯»å–SQLæ–‡ä»¶
        print(f"ğŸ“– æ­£åœ¨è¯»å–SQLæ–‡ä»¶: {sql_file_path}")
        if not os.path.exists(sql_file_path):
            return False, f"æ–‡ä»¶ä¸å­˜åœ¨: {sql_file_path}"
        
        with open(sql_file_path, 'r', encoding='utf-8') as f:
            sql_content = f.read()
        
        # è‹¥æ–‡ä»¶ä¸ºç©ºï¼Œåˆ™ç›´æ¥è¿”å›æˆåŠŸ
        if not sql_content.strip():
            return True, ""
        
        # 2. è§£æSQLè¯­å¥
        print(f"ğŸ” æ­£åœ¨è§£æSQLè¯­å¥...")
        try:
            parsed_statements = sqlglot.parse(sql_content, dialect=dialect)
        except Exception as e:
            return False, f"SQLè§£æå¤±è´¥: {str(e)}"
        
        if not parsed_statements:
            return False, "æœªèƒ½è§£æå‡ºä»»ä½•SQLè¯­å¥"
        
        print(f"âœ… æˆåŠŸè§£æ {len(parsed_statements)} æ¡SQLè¯­å¥")
        
        # 3. æå–å…ƒæ•°æ®
        print(f"ğŸ“Š æ­£åœ¨æå–å…ƒæ•°æ®...")
        extracted_data = []
        
        for idx, parsed_sql in enumerate(parsed_statements, 1):
            if parsed_sql is None:
                continue
                
            try:
                statement_type = _identify_statement_type(parsed_sql)
                
                if statement_type == 'DDL':
                    print(f"  [{idx}] DDLè¯­å¥ - {type(parsed_sql).__name__}")
                    metadata = extract_ddl_metadata(parsed_sql.sql(dialect=dialect), dialect=dialect)
                    metadata['_type'] = 'DDL'
                    metadata['_ast'] = parsed_sql
                    extracted_data.append(metadata)
                    
                elif statement_type == 'DML':
                    print(f"  [{idx}] DMLè¯­å¥ - {type(parsed_sql).__name__}")
                    metadata = extract_sql_metadata(parsed_sql.sql(dialect=dialect), dialect=dialect)
                    metadata['_type'] = 'DML'
                    metadata['_ast'] = parsed_sql
                    extracted_data.append(metadata)
                    
                else:
                    print(f"  [{idx}] è·³è¿‡è¯­å¥ - {type(parsed_sql).__name__} (ä¸æ”¯æŒçš„ç±»å‹)")
                    
            except Exception as e:
                return False, f"æå–ç¬¬{idx}æ¡SQLå…ƒæ•°æ®å¤±è´¥: {str(e)}"
        
        if not extracted_data:
            return False, "æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„å…ƒæ•°æ®"
        
        print(f"âœ… æˆåŠŸæå– {len(extracted_data)} æ¡å…ƒæ•°æ®")
        
        # 4. æ•´åˆæ•°æ®ï¼ˆæŒ‰è¡¨åˆ†ç»„ï¼‰
        print(f"ğŸ”„ æ­£åœ¨æ•´åˆæ•°æ®...")
        tables_data = _consolidate_metadata(extracted_data)
        print(f"âœ… æ•´åˆåå…± {len(tables_data)} ä¸ªè¡¨")
        
        # 5. æ„å»ºä¾èµ–å›¾å¹¶è¯†åˆ«ç›®æ ‡è¡¨ï¼ˆéœ€è¦åœ¨å¤„ç†è¡¨æ•°æ®å‰ç”Ÿæˆscript_idï¼‰
        print(f"\nğŸ“Š æ­£åœ¨æ„å»ºä¾èµ–å›¾...")
        try:
            dependency_graph = _build_dependency_graph(extracted_data)
            print(f"âœ… ä¾èµ–å›¾æ„å»ºå®Œæˆ: {len(dependency_graph.nodes())} ä¸ªèŠ‚ç‚¹, {len(dependency_graph.edges())} æ¡è¾¹")
            
            # 6. ä¿å­˜ä¾èµ–å›¾åˆ°æ–‡ä»¶
            graph_file_path = _save_dependency_graph(sql_file_path, dependency_graph)
            print(f"âœ… ä¾èµ–å›¾å·²ä¿å­˜åˆ°: {graph_file_path}")
            
            # 7. è¯†åˆ«ç›®æ ‡è¡¨å’Œæ¥æºè¡¨
            target_tables, source_tables = _identify_target_and_source_tables(dependency_graph, extracted_data)
            print(f"âœ… è¯†åˆ«åˆ°ç›®æ ‡è¡¨: {target_tables}")
            print(f"âœ… è¯†åˆ«åˆ°æ¥æºè¡¨: {source_tables}")
            
            # æ£€æŸ¥ç›®æ ‡è¡¨æ•°é‡
            if len(target_tables) == 0:
                return False, "æœªèƒ½è¯†åˆ«åˆ°ç›®æ ‡è¡¨"
            
            # 8. ç”Ÿæˆscript_idï¼ˆåªä½¿ç”¨è„šæœ¬åï¼Œä¸å«æ‰©å±•åï¼‰
            script_name = os.path.splitext(os.path.basename(sql_file_path))[0]
            script_id = script_name
            
            print(f"âœ… ç”Ÿæˆè„šæœ¬ID: {script_id}")
            print(f"   è„šæœ¬æ“ä½œ {len(target_tables)} ä¸ªç›®æ ‡è¡¨")
            
        except Exception as e:
            return False, f"æ„å»ºä¾èµ–å›¾å¤±è´¥: {str(e)}"
        
        # 9. è¿æ¥æ•°æ®åº“å¹¶å¤„ç†å†²çª
        print(f"\nğŸ’¾ æ­£åœ¨è¿æ¥æ•°æ®åº“: {db_path}")
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row  # ä½¿ç»“æœå¯ä»¥é€šè¿‡åˆ—åè®¿é—®
        
        try:
            # ä½¿ç”¨äº‹åŠ¡
            cursor = conn.cursor()
            cursor.execute("BEGIN TRANSACTION")
            
            # 10. å¤„ç†æ¯ä¸ªè¡¨çš„æ•°æ®ï¼ˆä¼ å…¥script_idç”¨äºä¸´æ—¶è¡¨ï¼‰
            for table_key, table_data in tables_data.items():
                print(f"\nğŸ“‹ å¤„ç†è¡¨: {table_key}")
                try:
                    _process_table_data(cursor, table_data, script_id)
                except Exception as e:
                    conn.rollback()
                    return False, f"å¤„ç†è¡¨ {table_key} æ—¶å‡ºé”™: {str(e)}"
            
            # 11. å¡«å……sql_scriptsã€data_lineageã€script_dependenciesè¡¨
            print(f"\nğŸ“ æ­£åœ¨å¡«å……è„šæœ¬ä¿¡æ¯...")
            _populate_script_tables(
                cursor, 
                sql_file_path, 
                sql_content,
                target_tables,  # ä¼ å…¥ç›®æ ‡è¡¨é›†åˆï¼ˆå¯èƒ½å¤šä¸ªï¼‰
                source_tables,
                extracted_data,
                dependency_graph
            )
            print(f"âœ… è„šæœ¬ä¿¡æ¯å·²ä¿å­˜")
            
            # 12. æ›´æ–°å…¨å±€è¡€ç¼˜å›¾
            print(f"\nğŸŒ æ­£åœ¨æ›´æ–°å…¨å±€è¡€ç¼˜å›¾...")
            _update_global_lineage(
                sql_file_path,
                target_tables,  # ä¼ å…¥ç›®æ ‡è¡¨é›†åˆï¼ˆå¯èƒ½å¤šä¸ªï¼‰
                source_tables
            )
            print(f"âœ… å…¨å±€è¡€ç¼˜å›¾å·²æ›´æ–°")
            
            # æäº¤äº‹åŠ¡
            conn.commit()
            print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“")
            
        finally:
            conn.close()
        
        return True, ""
        
    except Exception as e:
        return False, f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}"


def _identify_statement_type(parsed_sql: exp.Expression) -> str:
    """
    è¯†åˆ«SQLè¯­å¥ç±»å‹
    
    Args:
        parsed_sql: sqlglotè§£æåçš„AST
    
    Returns:
        'DDL' - CREATEè¯­å¥
        'DML' - INSERT/UPDATE/MERGEè¯­å¥
        'OTHER' - å…¶ä»–ç±»å‹
    """
    if isinstance(parsed_sql, exp.Create):
        return 'DDL'
    elif isinstance(parsed_sql, (exp.Insert, exp.Update, exp.Merge)):
        return 'DML'
    else:
        return 'OTHER'


def _consolidate_metadata(extracted_data: List[Dict]) -> Dict[str, Dict]:
    """
    æ•´åˆå…ƒæ•°æ®ï¼ˆæŒ‰è¡¨åˆ†ç»„ï¼‰
    
    å¤„ç†ç›®æ ‡è¡¨å’Œæ¥æºè¡¨ï¼Œç¡®ä¿æ‰€æœ‰å¼•ç”¨çš„è¡¨éƒ½è¢«è®°å½•
    
    Args:
        extracted_data: æå–çš„å…ƒæ•°æ®åˆ—è¡¨
    
    Returns:
        æŒ‰è¡¨åˆ†ç»„çš„æ•°æ®å­—å…¸ï¼Œkeyä¸º(schema_name, table_name)
    """
    tables_data = {}
    
    for metadata in extracted_data:
        stmt_type = metadata['_type']
        target_table = metadata['target_table']
        
        # 1. å¤„ç†ç›®æ ‡è¡¨
        schema_name = target_table.get('schema_nm', '') or ''
        table_name = target_table.get('tbl_en_nm', '')
        
        if table_name:
            # ä½¿ç”¨(schema_name, table_name)ä½œä¸ºkey
            table_key = (schema_name, table_name)
            
            if table_key not in tables_data:
                # ç¡®å®šè¡¨ç±»å‹
                table_type = _determine_table_type(metadata['_ast'], schema_name)
                tables_data[table_key] = {
                    'schema_name': schema_name,
                    'table_name': table_name,
                    'table_cn_name': target_table.get('tbl_cn_nm', ''),
                    'table_type': table_type,
                    'data_source': stmt_type,
                    'columns': [],
                    'ast': metadata['_ast']
                }
            
            # å¦‚æœå½“å‰è®°å½•æ˜¯DDLï¼Œæ›´æ–°data_source
            if stmt_type == 'DDL':
                tables_data[table_key]['data_source'] = 'DDL'
                tables_data[table_key]['table_cn_name'] = target_table.get('tbl_cn_nm', '')
            
            # æ•´åˆå­—æ®µä¿¡æ¯
            if 'target_columns' in metadata and metadata['target_columns']:
                for col in metadata['target_columns']:
                    # æ£€æŸ¥å­—æ®µæ˜¯å¦å·²å­˜åœ¨
                    existing_col = None
                    for existing in tables_data[table_key]['columns']:
                        if existing['col_en_nm'] == col.get('col_en_nm'):
                            existing_col = existing
                            break
                    
                    if existing_col:
                        # åˆå¹¶å­—æ®µä¿¡æ¯ï¼ˆä¼˜å…ˆä¿ç•™DDLä¿¡æ¯ï¼‰
                        if stmt_type == 'DDL':
                            # DDLä¼˜å…ˆï¼Œè¦†ç›–åŸæœ‰ä¿¡æ¯
                            for key, value in col.items():
                                if value or key in ['is_null', 'is_pri_key', 'is_foreign_key']:
                                    existing_col[key] = value
                        else:
                            # DMLè¡¥å……ä¿¡æ¯
                            for key, value in col.items():
                                if value and not existing_col.get(key):
                                    existing_col[key] = value
                    else:
                        # æ–°å­—æ®µ
                        tables_data[table_key]['columns'].append(col)
        
        # 2. å¤„ç†æ¥æºè¡¨ï¼ˆä»…å½“æ˜¯DMLæˆ–æœ‰source_tablesæ—¶ï¼‰
        if 'source_tables' in metadata and metadata['source_tables']:
            for source_table in metadata['source_tables']:
                src_schema = source_table.get('schema_nm', '') or ''
                src_table = source_table.get('tbl_en_nm', '')
                
                if not src_table:
                    continue
                
                src_key = (src_schema, src_table)
                
                # å¦‚æœæ¥æºè¡¨è¿˜æœªè®°å½•ï¼Œæ·»åŠ ä¸ºå¤–éƒ¨è¡¨
                if src_key not in tables_data:
                    tables_data[src_key] = {
                        'schema_name': src_schema,
                        'table_name': src_table,
                        'table_cn_name': '',
                        'table_type': 'TABLE',  # é»˜è®¤ä¸ºTABLEç±»å‹
                        'data_source': 'EXTERNAL',  # æ ‡è®°ä¸ºå¤–éƒ¨è¡¨
                        'columns': [],
                        'ast': None
                    }
    
    return tables_data


def _process_table_data(cursor: sqlite3.Cursor, table_data: Dict, script_id: str = None):
    """
    å¤„ç†å•ä¸ªè¡¨çš„æ•°æ®ï¼ˆåŒ…æ‹¬å†²çªæ£€æµ‹å’Œåˆå¹¶ï¼‰
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        table_data: è¡¨æ•°æ®å­—å…¸
    
    Raises:
        Exception: å½“æ£€æµ‹åˆ°ä¸å…è®¸çš„å†²çªæ—¶
    """
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    table_cn_name = table_data['table_cn_name']
    data_source = table_data['data_source']
    table_type = table_data['table_type']
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    is_tmp_table = (table_type == 'TMP_TABLE')
    # script_id: å®ä½“è¡¨ä¸ºNoneï¼Œä¸´æ—¶è¡¨ä¸ºå®é™…å€¼ï¼ˆç”¨äºIDç”Ÿæˆå’Œé€»è¾‘åˆ¤æ–­ï¼‰
    current_script_id = script_id if is_tmp_table else None

    # ç”Ÿæˆè¡¨IDï¼ˆä¸´æ—¶è¡¨éœ€è¦ä¼ å…¥script_idï¼‰
    table_id = _generate_table_id(schema_name, table_name, current_script_id)

    # æŸ¥è¯¢æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥è¡¨
    cursor.execute("""
        SELECT id, schema_name, table_name, table_type, description,
               data_source, refresh_frequency, row_count, data_size_mb,
               last_updated, created_at, script_id
        FROM tables
        WHERE id = ?
    """, (table_id,))

    existing_table = cursor.fetchone()
    
    if existing_table:
        print(f"  âš ï¸  è¡¨å·²å­˜åœ¨ï¼Œæ£€æµ‹å†²çª...")
        existing_data_source = existing_table['data_source']
        
        # å¦‚æœæ–°æ•°æ®æ˜¯EXTERNALï¼Œè·³è¿‡ï¼ˆå·²æœ‰ä»»ä½•å®šä¹‰éƒ½ä¼˜å…ˆï¼‰
        if data_source == 'EXTERNAL':
            print(f"  â­ï¸  è¡¨å·²å­˜åœ¨ï¼Œè·³è¿‡å¤–éƒ¨è¡¨åˆ›å»º")
            return
        
        # å¦‚æœå·²å­˜åœ¨çš„æ˜¯EXTERNALï¼Œç”¨æ–°æ•°æ®è¦†ç›–
        if existing_data_source == 'EXTERNAL':
            print(f"  ğŸ”„ ç”¨å®é™…å®šä¹‰è¦†ç›–å¤–éƒ¨è¡¨è®°å½•")
            if data_source == 'DDL':
                _update_table_with_ddl(cursor, table_id, table_data, current_script_id)
            else:  # DML
                _update_table_with_ddl(cursor, table_id, table_data, current_script_id)
            return
        
        # åœºæ™¯1: DDL vs DDL - æŠ¥é”™
        if existing_data_source == 'DDL' and data_source == 'DDL':
            raise Exception(f"è¡¨ {schema_name}.{table_name} å·²å­˜åœ¨DDLå®šä¹‰ï¼Œä¸å…è®¸é‡å¤å®šä¹‰")
        
        # åœºæ™¯2: DML vs DDL - DDLè¦†ç›–
        elif existing_data_source == 'DML' and data_source == 'DDL':
            print(f"  ğŸ”„ DMLè¡¨è¢«DDLè¦†ç›–")
            _update_table_with_ddl(cursor, table_id, table_data, current_script_id)

        # åœºæ™¯3: DDL vs DML - DDLä¿æŒï¼Œè¡¥å……ä¿¡æ¯
        elif existing_data_source == 'DDL' and data_source == 'DML':
            print(f"  â• è¡¥å……DMLä¿¡æ¯åˆ°DDLè¡¨")
            _supplement_ddl_with_dml(cursor, table_id, table_data, current_script_id)

        # åœºæ™¯4: DML vs DML - åˆå¹¶
        elif existing_data_source == 'DML' and data_source == 'DML':
            print(f"  ğŸ”€ åˆå¹¶DMLä¿¡æ¯")
            _merge_dml_with_dml(cursor, table_id, table_data, current_script_id)

    else:
        print(f"  âœ¨ æ–°å»ºè¡¨è®°å½•")
        _insert_new_table(cursor, table_data, current_script_id)


def _generate_table_id(schema_name: str, table_name: str, script_id: str = None) -> str:
    """
    ç”Ÿæˆè¡¨ID
    è§„åˆ™ï¼š
    - æœ‰schemaä¸”æœ‰script_idï¼ˆä¸´æ—¶è¡¨ï¼‰: {SCHEMA_NAME}__{TABLE_NAME}__{SCRIPT_ID}
    - æœ‰schemaæ— script_idï¼ˆå®ä½“è¡¨ï¼‰: {SCHEMA_NAME}__{TABLE_NAME}__
    - æ— schemaæœ‰script_idï¼ˆä¸´æ—¶è¡¨ï¼‰: __{TABLE_NAME}__{SCRIPT_ID}
    - æ— schemaæ— script_idï¼ˆä¸´æ—¶è¡¨ï¼Œæ— è„šæœ¬ï¼‰: __{TABLE_NAME}__
    """
    if schema_name:
        if script_id:
            return f"{schema_name}__{table_name}__{script_id}"
        else:
            return f"{schema_name}__{table_name}__"
    else:
        if script_id:
            return f"__{table_name}__{script_id}"
        else:
            return f"__{table_name}__"


def _generate_column_id(schema_name: str, table_name: str, column_name: str, script_id: str = None) -> str:
    """
    ç”Ÿæˆå­—æ®µID
    è§„åˆ™ï¼š
    - æœ‰schemaä¸”æœ‰script_idï¼ˆä¸´æ—¶è¡¨ï¼‰: {SCHEMA_NAME}__{TABLE_NAME}__{SCRIPT_ID}__{COLUMN_NAME}
    - æœ‰schemaæ— script_idï¼ˆå®ä½“è¡¨ï¼‰: {SCHEMA_NAME}__{TABLE_NAME}____{COLUMN_NAME}
    - æ— schemaæœ‰script_idï¼ˆä¸´æ—¶è¡¨ï¼‰: __{TABLE_NAME}__{SCRIPT_ID}__{COLUMN_NAME}
    - æ— schemaæ— script_idï¼ˆä¸´æ—¶è¡¨ï¼Œæ— è„šæœ¬ï¼‰: __{TABLE_NAME}____{COLUMN_NAME}
    """
    if schema_name:
        if script_id:
            return f"{schema_name}__{table_name}__{script_id}__{column_name}"
        else:
            return f"{schema_name}__{table_name}____{column_name}"
    else:
        if script_id:
            return f"__{table_name}__{script_id}__{column_name}"
        else:
            return f"__{table_name}____{column_name}"


def _determine_table_type(ast: exp.Expression, schema_name: str) -> str:
    """
    ç¡®å®šè¡¨ç±»å‹
    
    Args:
        ast: SQLè¯­å¥çš„AST
        schema_name: schemaåç§°
    
    Returns:
        'TABLE', 'VIEW', 'TMP_TABLE'
    """
    if isinstance(ast, exp.Create):
        # æ£€æŸ¥æ˜¯å¦æ˜¯VIEW
        if hasattr(ast, 'kind') and ast.kind and 'VIEW' in str(ast.kind).upper():
            return 'VIEW'
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶è¡¨
        if hasattr(ast, 'args'):
            # TEMPORARY TABLE
            if ast.args.get('temporary'):
                return 'TMP_TABLE'
            
            # VOLATILE TABLE (Teradata) - æ£€æŸ¥argsä¸­çš„volatileæ ‡å¿—
            if ast.args.get('volatile'):
                return 'TMP_TABLE'
            
            # æ£€æŸ¥propertiesä¸­æ˜¯å¦åŒ…å«VOLATILEæˆ–TEMPORARY
            if 'properties' in ast.args and ast.args['properties']:
                properties = ast.args['properties']
                if hasattr(properties, 'expressions'):
                    for prop in properties.expressions:
                        prop_str = str(type(prop).__name__).upper()
                        prop_value = str(prop).upper()
                        # æ£€æŸ¥StabilityProperty: VOLATILE æˆ–åŒ…å«TEMPORARYçš„å±æ€§
                        if 'VOLATILE' in prop_str or 'VOLATILE' in prop_value:
                            return 'TMP_TABLE'
                        if 'TEMPORARY' in prop_str or 'TEMPORARY' in prop_value:
                            return 'TMP_TABLE'
            
            # æ£€æŸ¥kindä¸­æ˜¯å¦åŒ…å«VOLATILEæˆ–TEMPORARYå…³é”®è¯
            if hasattr(ast, 'kind') and ast.kind:
                kind_str = str(ast.kind).upper()
                if 'VOLATILE' in kind_str or 'TEMPORARY' in kind_str:
                    return 'TMP_TABLE'
        
        return 'TABLE'
    
    elif isinstance(ast, (exp.Insert, exp.Update, exp.Merge)):
        # DMLè¯­å¥ï¼šæœ‰schema_nameåˆ™ä¸ºTABLEï¼Œå¦åˆ™ä¸ºTMP_TABLE
        if schema_name:
            return 'TABLE'
        else:
            return 'TMP_TABLE'
    
    return 'TABLE'


def _insert_new_table(cursor: sqlite3.Cursor, table_data: Dict, script_id: str = None):
    """æ’å…¥æ–°è¡¨"""
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    table_cn_name = table_data['table_cn_name']
    data_source = table_data['data_source']
    table_type = table_data['table_type']
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    is_tmp_table = (table_type == 'TMP_TABLE')
    current_script_id = script_id if is_tmp_table else None

    # ç”Ÿæˆè¡¨IDï¼ˆä¸´æ—¶è¡¨éœ€è¦ä¼ å…¥script_idï¼‰
    table_id = _generate_table_id(schema_name, table_name, current_script_id)
    
    # å¤„ç†database_id
    database_id = ''
    if schema_name:
        # ç¡®ä¿databaseè®°å½•å­˜åœ¨
        cursor.execute("SELECT id FROM databases WHERE id = ?", (schema_name,))
        if not cursor.fetchone():
            cursor.execute("""
                INSERT INTO databases (id, name, description)
                VALUES (?, ?, '')
            """, (schema_name, schema_name))
            print(f"  ğŸ“‚ åˆ›å»ºæ•°æ®åº“è®°å½•: {schema_name}")
        database_id = schema_name
    
    # æ’å…¥è¡¨è®°å½•ï¼ˆæ–‡æœ¬å­—æ®µä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä»£æ›¿NULLï¼Œæ•°å€¼/æ—¥æœŸå­—æ®µä½¿ç”¨NULLï¼‰
    cursor.execute("""
        INSERT INTO tables (
            id, database_id, schema_name, table_name, table_type,
            description, business_purpose, data_source, refresh_frequency,
            row_count, data_size_mb, last_updated, script_id
        ) VALUES (?, ?, ?, ?, ?, ?, '', ?, 'DAILY', NULL, NULL, NULL, ?)
    """, (
        table_id,
        database_id,
        schema_name or '',
        table_name,
        table_type,
        table_cn_name or '',
        data_source,
        current_script_id or ''  # å°†Noneè½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
    ))
    
    print(f"  âœ… æ’å…¥è¡¨: {table_id} (ç±»å‹: {table_type})")
    
    # æ’å…¥å­—æ®µè®°å½•
    _insert_columns(cursor, table_id, schema_name, table_name, table_data['columns'], current_script_id)


def _update_table_with_ddl(cursor: sqlite3.Cursor, table_id: str, table_data: Dict, script_id: str = None):
    """ç”¨DDLè¦†ç›–DMLè¡¨"""
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    table_cn_name = table_data['table_cn_name']
    table_type = table_data['table_type']
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    is_tmp_table = (table_type == 'TMP_TABLE')
    current_script_id = script_id if is_tmp_table else None
    
    # è¯»å–ç°æœ‰å­—æ®µçš„ä¸­æ–‡åï¼ˆDMLå¯èƒ½æœ‰ï¼‰
    cursor.execute("""
        SELECT column_name, description
        FROM columns
        WHERE table_id = ? AND description IS NOT NULL AND description != ''
    """, (table_id,))
    
    existing_col_descriptions = {row['column_name']: row['description'] for row in cursor.fetchall()}
    
    # æ›´æ–°è¡¨ä¿¡æ¯ï¼ˆæ–‡æœ¬å­—æ®µä½¿ç”¨ç©ºå­—ç¬¦ä¸²ï¼‰
    cursor.execute("""
        UPDATE tables
        SET table_type = ?,
            description = ?,
            data_source = 'DDL'
        WHERE id = ?
    """, (table_type, table_cn_name or '', table_id))
    
    print(f"  âœ… æ›´æ–°è¡¨ä¸ºDDL")
    
    # åˆ é™¤æ—§å­—æ®µ
    cursor.execute("DELETE FROM columns WHERE table_id = ?", (table_id,))
    
    # æ’å…¥DDLå­—æ®µï¼Œè¡¥å……DMLçš„ä¸­æ–‡å
    for col in table_data['columns']:
        col_en_nm = col.get('col_en_nm')
        if col_en_nm in existing_col_descriptions:
            # å¦‚æœDDLæ²¡æœ‰ä¸­æ–‡åï¼Œä½¿ç”¨DMLçš„
            if not col.get('col_cn_nm'):
                col['col_cn_nm'] = existing_col_descriptions[col_en_nm]
    
    _insert_columns(cursor, table_id, schema_name, table_name, table_data['columns'], current_script_id)


def _supplement_ddl_with_dml(cursor: sqlite3.Cursor, table_id: str, table_data: Dict, script_id: str = None):
    """è¡¥å……DMLä¿¡æ¯åˆ°DDLè¡¨"""
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    
    # è¯»å–ç°æœ‰å­—æ®µ
    cursor.execute("""
        SELECT column_name, description
        FROM columns
        WHERE table_id = ?
    """, (table_id,))
    
    existing_columns = {row['column_name']: row['description'] for row in cursor.fetchall()}
    
    # æ£€æŸ¥DMLçš„å­—æ®µ
    for col in table_data['columns']:
        col_en_nm = col.get('col_en_nm')
        
        if col_en_nm not in existing_columns:
            # DMLæœ‰æ–°å­—æ®µï¼ŒDDLæ²¡æœ‰ - æŠ¥é”™
            raise Exception(
                f"DMLè¯­å¥å¼•ç”¨äº†DDLä¸­ä¸å­˜åœ¨çš„å­—æ®µ: "
                f"{schema_name}.{table_name}.{col_en_nm}"
            )
        
        # è¡¥å……ä¸­æ–‡å
        col_cn_nm = col.get('col_cn_nm')
        if col_cn_nm and not existing_columns[col_en_nm]:
            cursor.execute("""
                UPDATE columns
                SET description = ?
                WHERE table_id = ? AND column_name = ?
            """, (col_cn_nm, table_id, col_en_nm))
            print(f"    â• è¡¥å……å­—æ®µä¸­æ–‡å: {col_en_nm} -> {col_cn_nm}")


def _merge_dml_with_dml(cursor: sqlite3.Cursor, table_id: str, table_data: Dict, script_id: str = None):
    """åˆå¹¶DMLä¸DML"""
    schema_name = table_data['schema_name']
    table_name = table_data['table_name']
    table_type = table_data['table_type']
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    is_tmp_table = (table_type == 'TMP_TABLE')
    current_script_id = script_id if is_tmp_table else None
    
    # è¯»å–ç°æœ‰å­—æ®µ
    cursor.execute("""
        SELECT column_name, data_type, is_nullable, default_value,
               is_primary_key, is_foreign_key, description, ordinal_position
        FROM columns
        WHERE table_id = ?
    """, (table_id,))
    
    existing_columns = {row['column_name']: dict(row) for row in cursor.fetchall()}
    
    # å¤„ç†æ–°å­—æ®µ
    for col in table_data['columns']:
        col_en_nm = col.get('col_en_nm')
        
        if col_en_nm in existing_columns:
            # å­—æ®µå·²å­˜åœ¨ï¼Œæ£€æŸ¥å†²çª
            existing = existing_columns[col_en_nm]
            col_cn_nm = col.get('col_cn_nm')
            
            # æ£€æŸ¥å†²çªï¼šå¦‚æœä¸¤ä¸ªéƒ½æœ‰å€¼ä¸”ä¸åŒï¼Œåˆ™æŠ¥é”™
            if col_cn_nm and existing['description'] and col_cn_nm != existing['description']:
                raise Exception(
                    f"å­—æ®µä¸­æ–‡åå†²çª: {schema_name}.{table_name}.{col_en_nm} "
                    f"ç°æœ‰: '{existing['description']}', æ–°: '{col_cn_nm}'"
                )
            
            # ç”¨æœ‰å€¼è¦†ç›–æ— å€¼
            if col_cn_nm and not existing['description']:
                cursor.execute("""
                    UPDATE columns
                    SET description = ?
                    WHERE table_id = ? AND column_name = ?
                """, (col_cn_nm, table_id, col_en_nm))
                print(f"    ğŸ”„ æ›´æ–°å­—æ®µä¸­æ–‡å: {col_en_nm} -> {col_cn_nm}")
        
        else:
            # æ–°å­—æ®µï¼Œç›´æ¥æ·»åŠ ï¼ˆéœ€è¦ä¼ å…¥script_idä»¥æ”¯æŒä¸´æ—¶è¡¨ï¼‰
            col_id = _generate_column_id(schema_name, table_name, col_en_nm, current_script_id)
            cursor.execute("""
                INSERT INTO columns (
                    id, table_id, column_name, data_type, max_length,
                    is_nullable, default_value, is_primary_key, is_foreign_key,
                    description, ordinal_position
                ) VALUES (?, ?, ?, '', NULL, 1, '', 0, 0, ?, NULL)
            """, (
                col_id,
                table_id,
                col_en_nm,
                col.get('col_cn_nm') or ''
            ))
            print(f"    âœ¨ æ·»åŠ æ–°å­—æ®µ: {col_en_nm}")


def _insert_columns(cursor: sqlite3.Cursor, table_id: str, schema_name: str,
                   table_name: str, columns: List[Dict], script_id: str = None):
    """æ’å…¥å­—æ®µè®°å½•"""
    for col in columns:
        col_en_nm = col.get('col_en_nm')
        if not col_en_nm:
            continue

        col_id = _generate_column_id(schema_name, table_name, col_en_nm, script_id)
        
        # ä»colå­—å…¸ä¸­è·å–å€¼ï¼Œè®¾ç½®é»˜è®¤å€¼ï¼ˆæ–‡æœ¬å­—æ®µä½¿ç”¨ç©ºå­—ç¬¦ä¸²ï¼Œæ•°å€¼å­—æ®µä½¿ç”¨Noneï¼‰
        data_type = col.get('data_type') or ''
        max_length = None  # æš‚ä¸å¤„ç†ï¼Œæ•°å€¼å­—æ®µ
        is_nullable = 0 if col.get('is_null') == False else 1
        default_value = col.get('default_value') or ''
        is_primary_key = 1 if col.get('is_pri_key') else 0
        is_foreign_key = 1 if col.get('is_foreign_key') else 0
        description = col.get('col_cn_nm') or ''
        ordinal_position = col.get('col_no') or None  # æ•°å€¼å­—æ®µ
        
        cursor.execute("""
            INSERT INTO columns (
                id, table_id, column_name, data_type, max_length,
                is_nullable, default_value, is_primary_key, is_foreign_key,
                description, ordinal_position
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            col_id, table_id, col_en_nm, data_type, max_length,
            is_nullable, default_value, is_primary_key, is_foreign_key,
            description, ordinal_position
        ))
    
    print(f"  âœ… æ’å…¥ {len(columns)} ä¸ªå­—æ®µ")


def _build_dependency_graph(extracted_data: List[Dict]) -> nx.DiGraph:
    """
    æ„å»ºSQLä¾èµ–å›¾
    
    Args:
        extracted_data: æå–çš„å…ƒæ•°æ®åˆ—è¡¨
    
    Returns:
        NetworkXæœ‰å‘å›¾
    """
    graph = nx.DiGraph()
    
    for metadata in extracted_data:
        target_table = metadata['target_table']
        target_schema = target_table.get('schema_nm', '') or ''
        target_name = target_table.get('tbl_en_nm', '')
        
        if not target_name:
            continue
        
        # æ„é€ å®Œæ•´çš„è¡¨æ ‡è¯†
        target_full_name = f"{target_schema}.{target_name}" if target_schema else target_name
        
        # æ·»åŠ ç›®æ ‡è¡¨èŠ‚ç‚¹
        if target_full_name not in graph:
            graph.add_node(target_full_name, schema=target_schema, table=target_name)
        
        # æ·»åŠ æ¥æºè¡¨å’Œè¾¹ï¼ˆDMLæˆ–CREATE ASï¼‰
        # CREATE ASè¯­å¥ä¹Ÿæœ‰æ¥æºè¡¨ä¾èµ–
        if 'source_tables' in metadata and metadata['source_tables']:
            for source_table in metadata['source_tables']:
                source_schema = source_table.get('schema_nm', '') or ''
                source_name = source_table.get('tbl_en_nm', '')
                
                if not source_name:
                    continue
                
                # æ„é€ å®Œæ•´çš„æ¥æºè¡¨æ ‡è¯†
                source_full_name = f"{source_schema}.{source_name}" if source_schema else source_name
                
                # æ·»åŠ æ¥æºè¡¨èŠ‚ç‚¹
                if source_full_name not in graph:
                    graph.add_node(source_full_name, schema=source_schema, table=source_name)
                
                # æ·»åŠ è¾¹ï¼ˆæ¥æºè¡¨ -> ç›®æ ‡è¡¨ï¼‰ï¼Œé¿å…é‡å¤
                if not graph.has_edge(source_full_name, target_full_name):
                    graph.add_edge(source_full_name, target_full_name)
    
    return graph


def _save_dependency_graph(sql_file_path: str, graph: nx.DiGraph) -> str:
    """
    ä¿å­˜ä¾èµ–å›¾åˆ°JSONæ–‡ä»¶
    
    Args:
        sql_file_path: SQLæ–‡ä»¶è·¯å¾„
        graph: ä¾èµ–å›¾
    
    Returns:
        JSONæ–‡ä»¶è·¯å¾„
    """
    # ç”ŸæˆJSONæ–‡ä»¶å
    base_name = os.path.splitext(os.path.basename(sql_file_path))[0]
    dir_name = os.path.dirname(sql_file_path)
    if not dir_name:
        dir_name = '.'
    json_file_path = os.path.join(dir_name, f"{base_name}_graph.json")
    
    # ä½¿ç”¨networkxçš„node-linkæ ¼å¼å¯¼å‡º
    graph_data = nx.node_link_data(graph)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(json_file_path, 'w', encoding='utf-8') as f:
        json.dump(graph_data, f, ensure_ascii=False, indent=2)
    
    return json_file_path


def _identify_target_and_source_tables(
    graph: nx.DiGraph,
    extracted_data: List[Dict] = None
) -> Tuple[Set[str], Set[str]]:
    """
    è¯†åˆ«ç›®æ ‡è¡¨å’Œæ¥æºè¡¨
    
    ç›®æ ‡è¡¨è¯†åˆ«é€»è¾‘ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
    1. å…¥åº¦>0çš„éä¸´æ—¶è¡¨ï¼ˆæœ‰æ•°æ®æµå…¥çš„å®ä½“è¡¨ï¼‰
    2. å¦‚æœ1æœªæ‰¾åˆ°ï¼Œåˆ™æ‰¾å‡ºåº¦=0çš„è¡¨ï¼ˆæœ€ç»ˆèŠ‚ç‚¹ï¼‰
    3. å¦‚æœä»æœªæ‰¾åˆ°ï¼Œè¿”å›ç©ºé›†åˆ
    
    æ¥æºè¡¨: å…¥åº¦ä¸º0çš„è¡¨
    
    Args:
        graph: ä¾èµ–å›¾
        extracted_data: æå–çš„å…ƒæ•°æ®åˆ—è¡¨
    
    Returns:
        (ç›®æ ‡è¡¨é›†åˆ, æ¥æºè¡¨é›†åˆ)
    """
    target_tables = set()
    source_tables = set()
    
    # è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    def is_temp_table(table_name: str) -> bool:
        """
        ä¸´æ—¶è¡¨åˆ¤æ–­è§„åˆ™ï¼š
        1. æ²¡æœ‰schemaï¼ˆä¸åŒ…å«'.'ï¼‰
        2. æˆ–è€…ä»¥å¸¸è§ä¸´æ—¶è¡¨å‰ç¼€å¼€å¤´ï¼ˆVT_ã€TMP_ã€TEMP_ç­‰ï¼‰
        """
        if '.' not in table_name:
            # æ²¡æœ‰schemaçš„è¡¨ï¼Œå¯èƒ½æ˜¯ä¸´æ—¶è¡¨
            # è¿›ä¸€æ­¥æ£€æŸ¥è¡¨åç‰¹å¾
            table_only = table_name.upper()
            temp_prefixes = ['VT_', 'TMP_', 'TEMP_', 'VOLATILE_', '#']
            return any(table_only.startswith(prefix) for prefix in temp_prefixes)
        return False
    
    # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„åº¦æ•°ä¿¡æ¯
    nodes_info = []
    for node in graph.nodes():
        out_degree = graph.out_degree(node)
        in_degree = graph.in_degree(node)
        is_temp = is_temp_table(node)
        nodes_info.append({
            'node': node,
            'in_degree': in_degree,
            'out_degree': out_degree,
            'is_temp': is_temp
        })
    
    # ç­–ç•¥1ï¼šæ‰¾å…¥åº¦>0çš„éä¸´æ—¶è¡¨
    for info in nodes_info:
        if info['in_degree'] > 0 and not info['is_temp']:
            target_tables.add(info['node'])
    
    # ç­–ç•¥2ï¼šå¦‚æœç­–ç•¥1æœªæ‰¾åˆ°ï¼Œæ‰¾å‡ºåº¦=0çš„è¡¨
    if not target_tables:
        for info in nodes_info:
            if info['out_degree'] == 0:
                target_tables.add(info['node'])
    
    # è¯†åˆ«æ¥æºè¡¨ï¼ˆå…¥åº¦=0ï¼‰
    for info in nodes_info:
        if info['in_degree'] == 0:
            source_tables.add(info['node'])
    
    return target_tables, source_tables


def _create_external_table_record(cursor: sqlite3.Cursor, schema_name: str, table_name: str):
    """
    åˆ›å»ºå¤–éƒ¨è¡¨çš„åŸºç¡€è®°å½•
    
    å¤–éƒ¨è¡¨æ˜¯æŒ‡åœ¨å½“å‰è„šæœ¬ä¸­è¢«å¼•ç”¨ä½†æœªå®šä¹‰çš„è¡¨ï¼ˆé€šå¸¸æ˜¯å…¶ä»–ç³»ç»Ÿçš„è¡¨ï¼‰
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        schema_name: Schemaåç§°
        table_name: è¡¨å
    """
    table_id = _generate_table_id(schema_name, table_name, None)
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
    cursor.execute("SELECT id FROM tables WHERE id = ?", (table_id,))
    if cursor.fetchone():
        return  # å·²å­˜åœ¨ï¼Œä¸é‡å¤åˆ›å»º
    
    # è·å–æˆ–åˆ›å»ºdatabase_id
    database_id = schema_name if schema_name else ''
    if schema_name:
        cursor.execute("SELECT id FROM databases WHERE id = ?", (database_id,))
        if not cursor.fetchone():
            cursor.execute("""
                INSERT INTO databases (id, name, description) 
                VALUES (?, ?, '')
            """, (database_id, schema_name))
    
    # æ’å…¥å¤–éƒ¨è¡¨è®°å½•
    cursor.execute("""
        INSERT INTO tables (
            id, database_id, schema_name, table_name, table_type,
            table_cn_name, description, business_purpose, data_source,
            refresh_frequency, row_count, data_size_mb, script_id
        ) VALUES (?, ?, ?, ?, ?, '', 'å¤–éƒ¨è¡¨ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰', '', 'EXTERNAL', '', NULL, NULL, '')
    """, (
        table_id,
        database_id,
        schema_name,
        table_name,
        'TABLE'  # å¤–éƒ¨è¡¨é»˜è®¤ä¸ºTABLEç±»å‹
    ))


def _populate_script_tables(
    cursor: sqlite3.Cursor,
    sql_file_path: str,
    sql_content: str,
    target_tables: Set[str],
    source_tables: Set[str],
    extracted_data: List[Dict],
    dependency_graph: nx.DiGraph
):
    """
    å¡«å……sql_scriptsã€data_lineageã€script_dependenciesè¡¨
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        sql_file_path: SQLæ–‡ä»¶è·¯å¾„
        sql_content: SQLæ–‡ä»¶å†…å®¹
        target_tables: ç›®æ ‡è¡¨é›†åˆï¼ˆå®Œæ•´åç§°ï¼Œå¦‚schema.tableï¼‰
        source_tables: æ¥æºè¡¨é›†åˆ
        extracted_data: æå–çš„å…ƒæ•°æ®
        dependency_graph: ä¾èµ–å›¾
    """
    # ç”Ÿæˆscript_idå’Œscript_nameï¼ˆåªä½¿ç”¨è„šæœ¬åï¼Œä¸å«æ‰©å±•åï¼‰
    script_name = os.path.splitext(os.path.basename(sql_file_path))[0]
    script_id = script_name
    
    # 1. æ’å…¥sql_scriptsè¡¨ï¼ˆä¸€ä¸ªè„šæœ¬åªæœ‰ä¸€æ¡è®°å½•ï¼‰
    cursor.execute("""
        INSERT OR REPLACE INTO sql_scripts (
            id, script_name, script_content,
            script_type, script_purpose, author, description,
            execution_frequency, execution_order, is_active,
            last_executed, avg_execution_time_seconds, performance_stats_json
        ) VALUES (?, ?, ?, '', '', '', '', 'DAILY', NULL, 1, NULL, NULL, NULL)
    """, (
        script_id,
        script_name,
        sql_content
    ))
    
    # 2. å¡«å……data_lineageè¡¨ - ä¸ºæ¯ä¸ªç›®æ ‡è¡¨å’Œæ¯ä¸ªæ¥æºè¡¨çš„ç»„åˆåˆ›å»ºä¸€æ¡è®°å½•
    for target_table_full_name in target_tables:
        target_schema, target_name = _parse_full_table_name(target_table_full_name)
        
        # å°è¯•æŸ¥æ‰¾ç›®æ ‡è¡¨ï¼ˆå…ˆå°è¯•å®ä½“è¡¨ï¼Œå†å°è¯•ä¸´æ—¶è¡¨ï¼‰
        target_table_id = _generate_table_id(target_schema, target_name, None)
        cursor.execute("SELECT id FROM tables WHERE id = ?", (target_table_id,))
        
        if not cursor.fetchone():
            # å¦‚æœå®ä½“è¡¨ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾ä¸´æ—¶è¡¨ï¼ˆä½¿ç”¨å½“å‰script_idï¼‰
            target_table_id = _generate_table_id(target_schema, target_name, script_id)
            cursor.execute("SELECT id FROM tables WHERE id = ?", (target_table_id,))
            
            if not cursor.fetchone():
                print(f"  âš ï¸  ç›®æ ‡è¡¨ {target_table_full_name} ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œè·³è¿‡æ­¤ç›®æ ‡è¡¨çš„è¡€ç¼˜å…³ç³»")
                continue
        
        # ä¸ºå½“å‰ç›®æ ‡è¡¨åˆ›å»ºä¸æ‰€æœ‰æ¥æºè¡¨çš„è¡€ç¼˜å…³ç³»
        for source_table_full_name in source_tables:
            source_schema, source_name = _parse_full_table_name(source_table_full_name)
            
            # å°è¯•æŸ¥æ‰¾æ¥æºè¡¨ï¼ˆå…ˆå°è¯•å®ä½“è¡¨ï¼Œå†å°è¯•ä¸´æ—¶è¡¨ï¼‰
            source_table_id = _generate_table_id(source_schema, source_name, None)
            cursor.execute("SELECT id FROM tables WHERE id = ?", (source_table_id,))
            
            if not cursor.fetchone():
                # å¦‚æœå®ä½“è¡¨ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾ä¸´æ—¶è¡¨ï¼ˆä½¿ç”¨å½“å‰script_idï¼‰
                source_table_id = _generate_table_id(source_schema, source_name, script_id)
                cursor.execute("SELECT id FROM tables WHERE id = ?", (source_table_id,))
                
                if not cursor.fetchone():
                    # æ¥æºè¡¨ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªå¤–éƒ¨è¡¨è®°å½•
                    print(f"  ğŸ“¥ è‡ªåŠ¨åˆ›å»ºå¤–éƒ¨è¡¨è®°å½•: {source_table_full_name}")
                    _create_external_table_record(cursor, source_schema, source_name)
                    # é‡æ–°ç”Ÿæˆsource_table_idï¼ˆå¤–éƒ¨è¡¨æ²¡æœ‰script_idï¼‰
                    source_table_id = _generate_table_id(source_schema, source_name, None)
            
            # ç”Ÿæˆlineage_id
            lineage_id = f"{target_table_id}__{source_table_id}__{script_id}"
            
            # æ’å…¥æˆ–æ›´æ–°data_lineage
            cursor.execute("""
                INSERT OR REPLACE INTO data_lineage (
                    id, target_table_id, source_table_id, script_id,
                    lineage_type, transformation_logic, columns_mapping_json, filter_conditions
                ) VALUES (?, ?, ?, ?, '', '', '', '')
            """, (
                lineage_id,
                target_table_id,
                source_table_id,
                script_id
            ))
    
    # 3. å¡«å……script_dependenciesè¡¨ - ä¸ºæ¯ä¸ªæ¥æºè¡¨åˆ›å»ºä¸€æ¡è®°å½•
    for source_table_full_name in source_tables:
        source_schema, source_name = _parse_full_table_name(source_table_full_name)
        
        # å°è¯•æŸ¥æ‰¾æ¥æºè¡¨ï¼ˆå…ˆå°è¯•å®ä½“è¡¨ï¼Œå†å°è¯•ä¸´æ—¶è¡¨ï¼‰
        source_table_id = _generate_table_id(source_schema, source_name, None)
        cursor.execute("SELECT id FROM tables WHERE id = ?", (source_table_id,))
        
        if not cursor.fetchone():
            # å¦‚æœå®ä½“è¡¨ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾ä¸´æ—¶è¡¨ï¼ˆä½¿ç”¨å½“å‰script_idï¼‰
            source_table_id = _generate_table_id(source_schema, source_name, script_id)
            cursor.execute("SELECT id FROM tables WHERE id = ?", (source_table_id,))
            
            if not cursor.fetchone():
                # æ¥æºè¡¨ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»ºå¤–éƒ¨è¡¨è®°å½•ï¼ˆå¦‚æœè¿˜æœªåˆ›å»ºï¼‰
                _create_external_table_record(cursor, source_schema, source_name)
                # é‡æ–°ç”Ÿæˆsource_table_idï¼ˆå¤–éƒ¨è¡¨æ²¡æœ‰script_idï¼‰
                source_table_id = _generate_table_id(source_schema, source_name, None)
        
        # ç”Ÿæˆdependency_id
        dependency_id = f"{source_table_id}__{script_id}"
        
        # æ’å…¥æˆ–æ›´æ–°script_dependencies
        cursor.execute("""
            INSERT OR REPLACE INTO script_dependencies (
                id, script_id, source_table_id, dependency_type,
                usage_pattern, columns_used_json, join_conditions, filter_conditions
            ) VALUES (?, ?, ?, '', '', '', '', '')
        """, (
            dependency_id,
            script_id,
            source_table_id
        ))


def _update_global_lineage(
    sql_file_path: str,
    target_tables: Set[str],
    source_tables: Set[str],
    lineage_json_path: str = 'datalineage.json'
):
    """
    æ›´æ–°å…¨å±€è¡€ç¼˜å›¾ï¼ˆdatalineage.jsonï¼‰
    
    Args:
        sql_file_path: SQLæ–‡ä»¶è·¯å¾„
        target_tables: ç›®æ ‡è¡¨é›†åˆï¼ˆå®Œæ•´åç§°ï¼‰
        source_tables: æ¥æºè¡¨é›†åˆ
        lineage_json_path: è¡€ç¼˜å›¾JSONæ–‡ä»¶è·¯å¾„
    """
    # è¯»å–ç°æœ‰è¡€ç¼˜å›¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(lineage_json_path):
        with open(lineage_json_path, 'r', encoding='utf-8') as f:
            graph_data = json.load(f)
        global_graph = nx.node_link_graph(graph_data, directed=True)
    else:
        global_graph = nx.DiGraph()
    
    # ä¸ºæ¯ä¸ªç›®æ ‡è¡¨å¤„ç†è¡€ç¼˜å…³ç³»
    for target_table in target_tables:
        # æ·»åŠ ç›®æ ‡è¡¨èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if target_table not in global_graph:
            target_schema, target_name = _parse_full_table_name(target_table)
            global_graph.add_node(target_table, schema=target_schema, table=target_name)
        
        # æ·»åŠ æ¥æºè¡¨èŠ‚ç‚¹å’Œè¾¹
        for source_table in source_tables:
            # æ·»åŠ æ¥æºè¡¨èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if source_table not in global_graph:
                source_schema, source_name = _parse_full_table_name(source_table)
                global_graph.add_node(source_table, schema=source_schema, table=source_name)
            
            # æ·»åŠ æˆ–æ›´æ–°è¾¹
            if global_graph.has_edge(source_table, target_table):
                # è¾¹å·²å­˜åœ¨ï¼Œæ›´æ–°script_pathså±æ€§
                edge_data = global_graph[source_table][target_table]
                script_paths = edge_data.get('script_paths', [])
                if sql_file_path not in script_paths:
                    script_paths.append(sql_file_path)
                    global_graph[source_table][target_table]['script_paths'] = script_paths
            else:
                # è¾¹ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è¾¹
                global_graph.add_edge(source_table, target_table, script_paths=[sql_file_path])
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    graph_data = nx.node_link_data(global_graph)
    with open(lineage_json_path, 'w', encoding='utf-8') as f:
        json.dump(graph_data, f, ensure_ascii=False, indent=2)


def _parse_full_table_name(full_name: str) -> Tuple[str, str]:
    """
    è§£æå®Œæ•´è¡¨åä¸º(schema, table)
    
    Args:
        full_name: å®Œæ•´è¡¨åï¼Œå¦‚"schema.table"æˆ–"table"
    
    Returns:
        (schema_name, table_name)
    """
    if '.' in full_name:
        parts = full_name.split('.')
        return parts[0], parts[1]
    else:
        return '', full_name


def process_sql_directory(
    directory_path: str,
    dialect: str = 'teradata',
    mode: str = 'insert',
    db_path: str = 'dw_metadata.db',
    lineage_json_path: str = 'datalineage.json',
    log_file: str = 'batch_process_log.txt'
) -> Dict:
    """
    æ‰¹é‡å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰SQLæ–‡ä»¶
    
    Args:
        directory_path: SQLæ–‡ä»¶ç›®å½•è·¯å¾„
        dialect: SQLæ–¹è¨€ï¼ˆå¦‚'mysql', 'teradata', 'postgres'ç­‰ï¼‰
        mode: å¤„ç†æ¨¡å¼
            - 'clear': æ¸…æ´—æ•°æ®åº“åå¤„ç†
            - 'insert': åœ¨å½“å‰æ•°æ®åŸºç¡€ä¸Šè¿½åŠ 
        db_path: SQLiteæ•°æ®åº“è·¯å¾„
        lineage_json_path: å…¨å±€è¡€ç¼˜å›¾JSONæ–‡ä»¶è·¯å¾„
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    
    Returns:
        {
            'success': True/False,
            'errors': [
                {'file': 'xxx.sql', 'error': 'xxx'}
            ]
        }
    """
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='w', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("="*70)
        logger.info("å¼€å§‹æ‰¹é‡å¤„ç†SQLæ–‡ä»¶")
        logger.info(f"ç›®å½•: {directory_path}")
        logger.info(f"æ–¹è¨€: {dialect}")
        logger.info(f"æ¨¡å¼: {mode}")
        logger.info("="*70)
        
        # 1. æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(directory_path):
            error_msg = f"ç›®å½•ä¸å­˜åœ¨: {directory_path}"
            logger.error(error_msg)
            return {'success': False, 'errors': [{'file': directory_path, 'error': error_msg}]}
        
        # 2. å¦‚æœæ˜¯clearæ¨¡å¼ï¼Œé‡æ–°åˆå§‹åŒ–æ•°æ®åº“
        if mode == 'clear':
            logger.info("\nğŸ”„ æ¨¡å¼: CLEAR - æ­£åœ¨é‡æ–°åˆå§‹åŒ–æ•°æ®åº“...")
            
            # åˆ é™¤æ—§çš„è¡€ç¼˜å›¾æ–‡ä»¶
            if os.path.exists(lineage_json_path):
                os.remove(lineage_json_path)
                logger.info(f"  å·²åˆ é™¤æ—§çš„è¡€ç¼˜å›¾æ–‡ä»¶: {lineage_json_path}")
            
            # è°ƒç”¨init_sqlite.pyé‡æ–°åˆå§‹åŒ–æ•°æ®åº“
            try:
                result = subprocess.run(
                    ['python', 'init_sqlite.py', '--force-reset'],
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    timeout=60
                )
                if result.returncode != 0:
                    error_msg = f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {result.stderr}"
                    logger.error(error_msg)
                    return {'success': False, 'errors': [{'file': 'init_sqlite.py', 'error': error_msg}]}
                logger.info("  âœ… æ•°æ®åº“å·²é‡æ–°åˆå§‹åŒ–")
            except subprocess.TimeoutExpired:
                error_msg = "æ•°æ®åº“åˆå§‹åŒ–è¶…æ—¶"
                logger.error(error_msg)
                return {'success': False, 'errors': [{'file': 'init_sqlite.py', 'error': error_msg}]}
            except Exception as e:
                error_msg = f"æ•°æ®åº“åˆå§‹åŒ–å¼‚å¸¸: {str(e)}"
                logger.error(error_msg)
                return {'success': False, 'errors': [{'file': 'init_sqlite.py', 'error': error_msg}]}
        
        # 3. é€’å½’æ‰«æç›®å½•ï¼Œè·å–æ‰€æœ‰SQLæ–‡ä»¶
        logger.info("\nğŸ“‚ æ­£åœ¨æ‰«æSQLæ–‡ä»¶...")
        sql_files = []
        for root, dirs, files in os.walk(directory_path):
            for file in files:
                if file.lower().endswith('.sql'):
                    sql_files.append(os.path.join(root, file))
        
        if not sql_files:
            logger.warning("  âš ï¸  æœªæ‰¾åˆ°ä»»ä½•SQLæ–‡ä»¶")
            return {'success': True, 'errors': []}
        
        logger.info(f"  âœ… æ‰¾åˆ° {len(sql_files)} ä¸ªSQLæ–‡ä»¶")
        
        # 4. é€ä¸ªå¤„ç†SQLæ–‡ä»¶
        logger.info("\nğŸ“Š å¼€å§‹å¤„ç†SQLæ–‡ä»¶...")
        errors = []
        
        for idx, sql_file in enumerate(sql_files, 1):
            relative_path = os.path.relpath(sql_file, directory_path)
            logger.info(f"\n[{idx}/{len(sql_files)}] å¤„ç†: {relative_path}")
            
            try:
                success, error_msg = process_sql_file(
                    sql_file_path=sql_file,
                    dialect=dialect,
                    db_path=db_path
                )
                
                if success:
                    logger.info(f"  âœ… æˆåŠŸ")
                else:
                    logger.error(f"  âŒ å¤±è´¥: {error_msg}")
                    errors.append({
                        'file': relative_path,
                        'error': error_msg
                    })
                    # ç«‹å³åœæ­¢æ¨¡å¼ï¼šé‡åˆ°é”™è¯¯ç›´æ¥è¿”å›
                    logger.error("\nâ›” é‡åˆ°é”™è¯¯ï¼Œåœæ­¢æ‰¹å¤„ç†")
                    return {'success': False, 'errors': errors}
                    
            except Exception as e:
                error_msg = f"å¤„ç†å¼‚å¸¸: {str(e)}"
                logger.error(f"  âŒ {error_msg}")
                errors.append({
                    'file': relative_path,
                    'error': error_msg
                })
                # ç«‹å³åœæ­¢æ¨¡å¼ï¼šé‡åˆ°é”™è¯¯ç›´æ¥è¿”å›
                logger.error("\nâ›” é‡åˆ°é”™è¯¯ï¼Œåœæ­¢æ‰¹å¤„ç†")
                return {'success': False, 'errors': errors}
        
        # 5. æ±‡æ€»ç»“æœ
        logger.info("\n" + "="*70)
        logger.info("æ‰¹å¤„ç†å®Œæˆ")
        logger.info(f"æ€»æ–‡ä»¶æ•°: {len(sql_files)}")
        logger.info(f"æˆåŠŸ: {len(sql_files) - len(errors)}")
        logger.info(f"å¤±è´¥: {len(errors)}")
        logger.info("="*70)
        
        if errors:
            logger.info("\nå¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
            for err in errors:
                logger.info(f"  âŒ {err['file']}")
                logger.info(f"     é”™è¯¯: {err['error']}")
        
        return {
            'success': len(errors) == 0,
            'errors': errors
        }
        
    except Exception as e:
        error_msg = f"æ‰¹å¤„ç†è¿‡ç¨‹å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}"
        logger.error(error_msg)
        return {'success': False, 'errors': [{'file': 'batch_process', 'error': error_msg}]}


# ä¸»ç¨‹åºå…¥å£ï¼ˆç”¨äºæµ‹è¯•ï¼‰
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        # print("ç”¨æ³•: python sql_file_processor.py <sql_file_path> [dialect]")
        # print("ç¤ºä¾‹: python sql_file_processor.py test.sql mysql")
        # sys.exit(1)
        
        sql_file = "C:\\pyworks\\Datasets\\SQLs\\DML\\Teradata\\minsheng\\MDB_TD\\sqls\\ccr88_eco_f_ivt_prd_ofl_dbt_if_mdm_10200.pl.12.sql"
        dialect = "teradata"

    else:
        sql_file = sys.argv[1]
        dialect = sys.argv[2] if len(sys.argv) > 2 else None
    
    success, error_msg = process_sql_file(sql_file, dialect=dialect)
    
    if success:
        print("\nğŸ‰ å¤„ç†æˆåŠŸï¼")
        sys.exit(0)
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {error_msg}")
        sys.exit(1)

